{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82eb82cc-2f4a-4679-b7cd-3334b9748c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def list_files_by_type(folder_path, file_type):\n",
    "    filtered_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if fnmatch.fnmatch(file, f\"*.{file_type}\"):\n",
    "            filtered_files.append(os.path.join(folder_path, file))\n",
    "    return filtered_files\n",
    "\n",
    "def process_image(args):\n",
    "    file_path, label, transform = args  # Unpack the tuple\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    default_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    tensor = default_transform(image)\n",
    "    if transform:\n",
    "        tensor = transform(tensor)\n",
    "    return tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class CIFAKEDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def extract_index_and_category(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        pattern = r\"(\\d+)(?: \\((\\d+)\\))?\\..+\"\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            category = int(match.group(2)) if match.group(2) else 0\n",
    "            return index, category\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_folder(folder_path, label, category=None, transform=None, num_processes=1):\n",
    "        print(f\"Loading folder: {folder_path}\")\n",
    "        files = list_files_by_type(folder_path, \"jpg\")\n",
    "        if category is not None:\n",
    "            files = [file for file in files if CIFAKEDataset.extract_index_and_category(file)[1] == category]\n",
    "\n",
    "        # Use process_map from tqdm.contrib.concurrent for better tqdm updates\n",
    "        results = thread_map(process_image, [(file, label, transform) for file in files], max_workers=num_processes, chunksize=1)\n",
    "\n",
    "        x = torch.stack([result[0] for result in results])\n",
    "        y = torch.stack([result[1] for result in results])\n",
    "        return x, y\n",
    "        \n",
    "    def __init__(self, folder_path, category=None, transform=None, num_processes=1):\n",
    "        label_1_folders = [\n",
    "            os.path.join(folder_path, \"train/REAL\"),\n",
    "            os.path.join(folder_path, \"test/REAL\"),\n",
    "        ]\n",
    "        label_0_folders = [\n",
    "            os.path.join(folder_path, \"train/FAKE\"),\n",
    "            os.path.join(folder_path, \"test/FAKE\"),\n",
    "        ]\n",
    "        x1, y1 = CIFAKEDataset.load_folder(label_1_folders[0], 1, category, transform, num_processes)\n",
    "        x2, y2 = CIFAKEDataset.load_folder(label_0_folders[0], 0, category, transform, num_processes)\n",
    "        x3, y3 = CIFAKEDataset.load_folder(label_1_folders[1], 1, category, transform, num_processes)\n",
    "        x4, y4 = CIFAKEDataset.load_folder(label_0_folders[1], 0, category, transform, num_processes)\n",
    "        self.x = torch.cat((x1, x2, x3, x4))\n",
    "        self.y = torch.cat((y1, y2, y3, y4))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def data_dim(self):\n",
    "        return self.x[0].size()\n",
    "    \n",
    "    def show_example(self, idx):\n",
    "        x, y = self[idx]\n",
    "        image_array = x.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f\"Label: {y}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0639d46e-3b0b-436e-bbcd-1b109a0caabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset...\n",
      "Loading folder: data/CIFAKE\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ae7a3f6bbc486ab9e6bd8a747c02df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7076f154a68488ab7292aabbcaf66a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abdace35f7c4507a193a2d25b24f467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c52379f69d14c7c8983619c067f773f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 120000\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = \"data/CIFAKE\"\n",
    "file_type = \"jpg\"\n",
    "print(\"Loading CIFAKEDataset...\")\n",
    "data_set = CIFAKEDataset(folder_path, num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(data_set))\n",
    "print(\"Data dimension:\", data_set.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "data_set.show_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617de45f-6a36-400b-978a-59438f1b482e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAKEClassifier(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network model following the architecture in the provided diagram\n",
    "class CIFAKEClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAKEClassifier, self).__init__()\n",
    "        # Assuming the input image size is 32x32x3 as per the rescale block in the diagram\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Convolutional layer with 32 outputs\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer with a 2x2 window and stride 2\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Second convolutional layer with 32 outputs\n",
    "        # Flatten layer will be applied in the forward pass\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, 1)  # Final dense layer with 1 unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU activation function after first convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU activation function after second convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the dense layer\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first dense layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply sigmoid activation function for binary classification\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CIFAKEClassifier()\n",
    "\n",
    "# Print the model structure\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139a1072-cb42-4016-9715-e7fda8ec5827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 0.6803\n",
      "Epoch 1, Batch 20, Loss: 0.6414\n",
      "Epoch 1, Batch 30, Loss: 0.6075\n",
      "Epoch 1, Batch 40, Loss: 0.5603\n",
      "Epoch 1, Batch 50, Loss: 0.4913\n",
      "Epoch 1, Batch 60, Loss: 0.5248\n",
      "Epoch 1, Batch 70, Loss: 0.4898\n",
      "Epoch 1, Batch 80, Loss: 0.4748\n",
      "Epoch 1, Batch 90, Loss: 0.4650\n",
      "Epoch 1, Batch 100, Loss: 0.4587\n",
      "Epoch 1, Batch 110, Loss: 0.4861\n",
      "Epoch 1, Batch 120, Loss: 0.4396\n",
      "Epoch 1, Batch 130, Loss: 0.4434\n",
      "Epoch 1, Batch 140, Loss: 0.4073\n",
      "Epoch 1, Batch 150, Loss: 0.4401\n",
      "Epoch 1, Batch 160, Loss: 0.4366\n",
      "Epoch 1, Batch 170, Loss: 0.4196\n",
      "Epoch 1, Batch 180, Loss: 0.4255\n",
      "Epoch 1, Batch 190, Loss: 0.3858\n",
      "Epoch 1, Batch 200, Loss: 0.3951\n",
      "Epoch 1, Batch 210, Loss: 0.3762\n",
      "Epoch 1, Batch 220, Loss: 0.3950\n",
      "Epoch 1, Batch 230, Loss: 0.3786\n",
      "Epoch 1, Batch 240, Loss: 0.3975\n",
      "Epoch 1, Batch 250, Loss: 0.4240\n",
      "Epoch 1, Batch 260, Loss: 0.3731\n",
      "Epoch 1, Batch 270, Loss: 0.3438\n",
      "Epoch 1, Batch 280, Loss: 0.3893\n",
      "Epoch 1, Batch 290, Loss: 0.3721\n",
      "Epoch 1, Batch 300, Loss: 0.3672\n",
      "Epoch 1, Batch 310, Loss: 0.3250\n",
      "Epoch 1, Batch 320, Loss: 0.3321\n",
      "Epoch 1, Batch 330, Loss: 0.3809\n",
      "Epoch 1, Batch 340, Loss: 0.3743\n",
      "Epoch 1, Batch 350, Loss: 0.3567\n",
      "Epoch 1, Batch 360, Loss: 0.4093\n",
      "Epoch 1, Batch 370, Loss: 0.3058\n",
      "Epoch 1, Batch 380, Loss: 0.3201\n",
      "Epoch 1, Batch 390, Loss: 0.3601\n",
      "Epoch 1, Batch 400, Loss: 0.3493\n",
      "Epoch 1, Batch 410, Loss: 0.3984\n",
      "Epoch 1, Batch 420, Loss: 0.3475\n",
      "Epoch 1, Batch 430, Loss: 0.3470\n",
      "Epoch 1, Batch 440, Loss: 0.3146\n",
      "Epoch 1, Batch 450, Loss: 0.3025\n",
      "Epoch 1, Batch 460, Loss: 0.4519\n",
      "Epoch 1, Batch 470, Loss: 0.4268\n",
      "Epoch 1, Batch 480, Loss: 0.3358\n",
      "Epoch 1, Batch 490, Loss: 0.3536\n",
      "Epoch 1, Batch 500, Loss: 0.3290\n",
      "Epoch 1, Batch 510, Loss: 0.2991\n",
      "Epoch 1, Batch 520, Loss: 0.3354\n",
      "Epoch 1, Batch 530, Loss: 0.3311\n",
      "Epoch 1, Batch 540, Loss: 0.2908\n",
      "Epoch 1, Batch 550, Loss: 0.3066\n",
      "Epoch 1, Batch 560, Loss: 0.2938\n",
      "Epoch 1, Batch 570, Loss: 0.3410\n",
      "Epoch 1, Batch 580, Loss: 0.3342\n",
      "Epoch 1, Batch 590, Loss: 0.3358\n",
      "Epoch 1, Batch 600, Loss: 0.3223\n",
      "Epoch 1, Batch 610, Loss: 0.2982\n",
      "Epoch 1, Batch 620, Loss: 0.3365\n",
      "Epoch 1, Batch 630, Loss: 0.3146\n",
      "Epoch 1, Batch 640, Loss: 0.3117\n",
      "Epoch 1, Batch 650, Loss: 0.3910\n",
      "Epoch 1, Batch 660, Loss: 0.3839\n",
      "Epoch 1, Batch 670, Loss: 0.3510\n",
      "Epoch 1, Batch 680, Loss: 0.3056\n",
      "Epoch 1, Batch 690, Loss: 0.3123\n",
      "Epoch 1, Batch 700, Loss: 0.3025\n",
      "Epoch 1, Batch 710, Loss: 0.2896\n",
      "Epoch 1, Batch 720, Loss: 0.3606\n",
      "Epoch 1, Batch 730, Loss: 0.3056\n",
      "Epoch 1, Batch 740, Loss: 0.3246\n",
      "Epoch 1, Batch 750, Loss: 0.3345\n",
      "Epoch 1, Batch 760, Loss: 0.3305\n",
      "Epoch 1, Batch 770, Loss: 0.2998\n",
      "Epoch 1, Batch 780, Loss: 0.3257\n",
      "Epoch 1, Batch 790, Loss: 0.3356\n",
      "Epoch 1, Batch 800, Loss: 0.2784\n",
      "Epoch 1, Batch 810, Loss: 0.2959\n",
      "Epoch 1, Batch 820, Loss: 0.3029\n",
      "Epoch 1, Batch 830, Loss: 0.2907\n",
      "Epoch 1, Batch 840, Loss: 0.2639\n",
      "Epoch 1, Batch 850, Loss: 0.3086\n",
      "Epoch 1, Batch 860, Loss: 0.3056\n",
      "Epoch 1, Batch 870, Loss: 0.2941\n",
      "Epoch 1, Batch 880, Loss: 0.2734\n",
      "Epoch 1, Batch 890, Loss: 0.3036\n",
      "Epoch 1, Batch 900, Loss: 0.2840\n",
      "Epoch 1, Batch 910, Loss: 0.2805\n",
      "Epoch 1, Batch 920, Loss: 0.2935\n",
      "Epoch 1, Batch 930, Loss: 0.2948\n",
      "Epoch 1, Batch 940, Loss: 0.2756\n",
      "Epoch 1, Batch 950, Loss: 0.2494\n",
      "Epoch 1, Batch 960, Loss: 0.2747\n",
      "Epoch 1, Batch 970, Loss: 0.3213\n",
      "Epoch 1, Batch 980, Loss: 0.2899\n",
      "Epoch 1, Batch 990, Loss: 0.2897\n",
      "Epoch 1, Batch 1000, Loss: 0.2872\n",
      "Epoch 1, Batch 1010, Loss: 0.2981\n",
      "Epoch 1, Batch 1020, Loss: 0.2439\n",
      "Epoch 1, Batch 1030, Loss: 0.2899\n",
      "Epoch 1, Batch 1040, Loss: 0.2763\n",
      "Epoch 1, Batch 1050, Loss: 0.2959\n",
      "Epoch 1, Batch 1060, Loss: 0.2923\n",
      "Epoch 1, Batch 1070, Loss: 0.2566\n",
      "Epoch 1, Batch 1080, Loss: 0.2520\n",
      "Epoch 1, Batch 1090, Loss: 0.2396\n",
      "Epoch 1, Batch 1100, Loss: 0.2814\n",
      "Epoch 1, Batch 1110, Loss: 0.2637\n",
      "Epoch 1, Batch 1120, Loss: 0.2821\n",
      "Epoch 1, Batch 1130, Loss: 0.2569\n",
      "Epoch 1, Batch 1140, Loss: 0.2523\n",
      "Epoch 1, Batch 1150, Loss: 0.2598\n",
      "Epoch 1, Batch 1160, Loss: 0.2741\n",
      "Epoch 1, Batch 1170, Loss: 0.2318\n",
      "Epoch 1, Batch 1180, Loss: 0.2930\n",
      "Epoch 1, Batch 1190, Loss: 0.2874\n",
      "Epoch 1, Batch 1200, Loss: 0.2635\n",
      "Epoch 1, Batch 1210, Loss: 0.2682\n",
      "Epoch 1, Batch 1220, Loss: 0.2518\n",
      "Epoch 1, Batch 1230, Loss: 0.2982\n",
      "Epoch 1, Batch 1240, Loss: 0.2904\n",
      "Epoch 1, Batch 1250, Loss: 0.2610\n",
      "Epoch 1, Batch 1260, Loss: 0.2518\n",
      "Epoch 1, Batch 1270, Loss: 0.2492\n",
      "Epoch 1, Batch 1280, Loss: 0.2714\n",
      "Epoch 1, Batch 1290, Loss: 0.2200\n",
      "Epoch 1, Batch 1300, Loss: 0.2354\n",
      "Epoch 1, Batch 1310, Loss: 0.2153\n",
      "Epoch 2, Batch 10, Loss: 0.2391\n",
      "Epoch 2, Batch 20, Loss: 0.2391\n",
      "Epoch 2, Batch 30, Loss: 0.1950\n",
      "Epoch 2, Batch 40, Loss: 0.2388\n",
      "Epoch 2, Batch 50, Loss: 0.2510\n",
      "Epoch 2, Batch 60, Loss: 0.2464\n",
      "Epoch 2, Batch 70, Loss: 0.2637\n",
      "Epoch 2, Batch 80, Loss: 0.2941\n",
      "Epoch 2, Batch 90, Loss: 0.2769\n",
      "Epoch 2, Batch 100, Loss: 0.2791\n",
      "Epoch 2, Batch 110, Loss: 0.2294\n",
      "Epoch 2, Batch 120, Loss: 0.2373\n",
      "Epoch 2, Batch 130, Loss: 0.2388\n",
      "Epoch 2, Batch 140, Loss: 0.2357\n",
      "Epoch 2, Batch 150, Loss: 0.2433\n",
      "Epoch 2, Batch 160, Loss: 0.2285\n",
      "Epoch 2, Batch 170, Loss: 0.2374\n",
      "Epoch 2, Batch 180, Loss: 0.2246\n",
      "Epoch 2, Batch 190, Loss: 0.2356\n",
      "Epoch 2, Batch 200, Loss: 0.2324\n",
      "Epoch 2, Batch 210, Loss: 0.2139\n",
      "Epoch 2, Batch 220, Loss: 0.2354\n",
      "Epoch 2, Batch 230, Loss: 0.2614\n",
      "Epoch 2, Batch 240, Loss: 0.2556\n",
      "Epoch 2, Batch 250, Loss: 0.2371\n",
      "Epoch 2, Batch 260, Loss: 0.2310\n",
      "Epoch 2, Batch 270, Loss: 0.2341\n",
      "Epoch 2, Batch 280, Loss: 0.2275\n",
      "Epoch 2, Batch 290, Loss: 0.2113\n",
      "Epoch 2, Batch 300, Loss: 0.2428\n",
      "Epoch 2, Batch 310, Loss: 0.1958\n",
      "Epoch 2, Batch 320, Loss: 0.2533\n",
      "Epoch 2, Batch 330, Loss: 0.2559\n",
      "Epoch 2, Batch 340, Loss: 0.2363\n",
      "Epoch 2, Batch 350, Loss: 0.2166\n",
      "Epoch 2, Batch 360, Loss: 0.2751\n",
      "Epoch 2, Batch 370, Loss: 0.2720\n",
      "Epoch 2, Batch 380, Loss: 0.2397\n",
      "Epoch 2, Batch 390, Loss: 0.1906\n",
      "Epoch 2, Batch 400, Loss: 0.2164\n",
      "Epoch 2, Batch 410, Loss: 0.1957\n",
      "Epoch 2, Batch 420, Loss: 0.2259\n",
      "Epoch 2, Batch 430, Loss: 0.2481\n",
      "Epoch 2, Batch 440, Loss: 0.2624\n",
      "Epoch 2, Batch 450, Loss: 0.1940\n",
      "Epoch 2, Batch 460, Loss: 0.2142\n",
      "Epoch 2, Batch 470, Loss: 0.2178\n",
      "Epoch 2, Batch 480, Loss: 0.2497\n",
      "Epoch 2, Batch 490, Loss: 0.2589\n",
      "Epoch 2, Batch 500, Loss: 0.2087\n",
      "Epoch 2, Batch 510, Loss: 0.2560\n",
      "Epoch 2, Batch 520, Loss: 0.2138\n",
      "Epoch 2, Batch 530, Loss: 0.1923\n",
      "Epoch 2, Batch 540, Loss: 0.1898\n",
      "Epoch 2, Batch 550, Loss: 0.2008\n",
      "Epoch 2, Batch 560, Loss: 0.2155\n",
      "Epoch 2, Batch 570, Loss: 0.2236\n",
      "Epoch 2, Batch 580, Loss: 0.2219\n",
      "Epoch 2, Batch 590, Loss: 0.2312\n",
      "Epoch 2, Batch 600, Loss: 0.2339\n",
      "Epoch 2, Batch 610, Loss: 0.2543\n",
      "Epoch 2, Batch 620, Loss: 0.2367\n",
      "Epoch 2, Batch 630, Loss: 0.2831\n",
      "Epoch 2, Batch 640, Loss: 0.2295\n",
      "Epoch 2, Batch 650, Loss: 0.2116\n",
      "Epoch 2, Batch 660, Loss: 0.2266\n",
      "Epoch 2, Batch 670, Loss: 0.2480\n",
      "Epoch 2, Batch 680, Loss: 0.2385\n",
      "Epoch 2, Batch 690, Loss: 0.2164\n",
      "Epoch 2, Batch 700, Loss: 0.2247\n",
      "Epoch 2, Batch 710, Loss: 0.2592\n",
      "Epoch 2, Batch 720, Loss: 0.2442\n",
      "Epoch 2, Batch 730, Loss: 0.2196\n",
      "Epoch 2, Batch 740, Loss: 0.2217\n",
      "Epoch 2, Batch 750, Loss: 0.2083\n",
      "Epoch 2, Batch 760, Loss: 0.2180\n",
      "Epoch 2, Batch 770, Loss: 0.2509\n",
      "Epoch 2, Batch 780, Loss: 0.2483\n",
      "Epoch 2, Batch 790, Loss: 0.2551\n",
      "Epoch 2, Batch 800, Loss: 0.2441\n",
      "Epoch 2, Batch 810, Loss: 0.2228\n",
      "Epoch 2, Batch 820, Loss: 0.1923\n",
      "Epoch 2, Batch 830, Loss: 0.2150\n",
      "Epoch 2, Batch 840, Loss: 0.2513\n",
      "Epoch 2, Batch 850, Loss: 0.2475\n",
      "Epoch 2, Batch 860, Loss: 0.2195\n",
      "Epoch 2, Batch 870, Loss: 0.2351\n",
      "Epoch 2, Batch 880, Loss: 0.2398\n",
      "Epoch 2, Batch 890, Loss: 0.2481\n",
      "Epoch 2, Batch 900, Loss: 0.2532\n",
      "Epoch 2, Batch 910, Loss: 0.2238\n",
      "Epoch 2, Batch 920, Loss: 0.1909\n",
      "Epoch 2, Batch 930, Loss: 0.2530\n",
      "Epoch 2, Batch 940, Loss: 0.2445\n",
      "Epoch 2, Batch 950, Loss: 0.2265\n",
      "Epoch 2, Batch 960, Loss: 0.2056\n",
      "Epoch 2, Batch 970, Loss: 0.1852\n",
      "Epoch 2, Batch 980, Loss: 0.2345\n",
      "Epoch 2, Batch 990, Loss: 0.2071\n",
      "Epoch 2, Batch 1000, Loss: 0.2014\n",
      "Epoch 2, Batch 1010, Loss: 0.2046\n",
      "Epoch 2, Batch 1020, Loss: 0.2130\n",
      "Epoch 2, Batch 1030, Loss: 0.1857\n",
      "Epoch 2, Batch 1040, Loss: 0.2253\n",
      "Epoch 2, Batch 1050, Loss: 0.2554\n",
      "Epoch 2, Batch 1060, Loss: 0.1917\n",
      "Epoch 2, Batch 1070, Loss: 0.2130\n",
      "Epoch 2, Batch 1080, Loss: 0.2272\n",
      "Epoch 2, Batch 1090, Loss: 0.1896\n",
      "Epoch 2, Batch 1100, Loss: 0.2286\n",
      "Epoch 2, Batch 1110, Loss: 0.2068\n",
      "Epoch 2, Batch 1120, Loss: 0.1613\n",
      "Epoch 2, Batch 1130, Loss: 0.2081\n",
      "Epoch 2, Batch 1140, Loss: 0.2195\n",
      "Epoch 2, Batch 1150, Loss: 0.1880\n",
      "Epoch 2, Batch 1160, Loss: 0.1948\n",
      "Epoch 2, Batch 1170, Loss: 0.1782\n",
      "Epoch 2, Batch 1180, Loss: 0.2251\n",
      "Epoch 2, Batch 1190, Loss: 0.2043\n",
      "Epoch 2, Batch 1200, Loss: 0.2525\n",
      "Epoch 2, Batch 1210, Loss: 0.1823\n",
      "Epoch 2, Batch 1220, Loss: 0.2009\n",
      "Epoch 2, Batch 1230, Loss: 0.2108\n",
      "Epoch 2, Batch 1240, Loss: 0.2079\n",
      "Epoch 2, Batch 1250, Loss: 0.2030\n",
      "Epoch 2, Batch 1260, Loss: 0.2061\n",
      "Epoch 2, Batch 1270, Loss: 0.1702\n",
      "Epoch 2, Batch 1280, Loss: 0.1887\n",
      "Epoch 2, Batch 1290, Loss: 0.1785\n",
      "Epoch 2, Batch 1300, Loss: 0.1949\n",
      "Epoch 2, Batch 1310, Loss: 0.2145\n",
      "Epoch 3, Batch 10, Loss: 0.2313\n",
      "Epoch 3, Batch 20, Loss: 0.2307\n",
      "Epoch 3, Batch 30, Loss: 0.1919\n",
      "Epoch 3, Batch 40, Loss: 0.1950\n",
      "Epoch 3, Batch 50, Loss: 0.1859\n",
      "Epoch 3, Batch 60, Loss: 0.2390\n",
      "Epoch 3, Batch 70, Loss: 0.1821\n",
      "Epoch 3, Batch 80, Loss: 0.1938\n",
      "Epoch 3, Batch 90, Loss: 0.1716\n",
      "Epoch 3, Batch 100, Loss: 0.1728\n",
      "Epoch 3, Batch 110, Loss: 0.1856\n",
      "Epoch 3, Batch 120, Loss: 0.1865\n",
      "Epoch 3, Batch 130, Loss: 0.1651\n",
      "Epoch 3, Batch 140, Loss: 0.2095\n",
      "Epoch 3, Batch 150, Loss: 0.2087\n",
      "Epoch 3, Batch 160, Loss: 0.1799\n",
      "Epoch 3, Batch 170, Loss: 0.2055\n",
      "Epoch 3, Batch 180, Loss: 0.1955\n",
      "Epoch 3, Batch 190, Loss: 0.2165\n",
      "Epoch 3, Batch 200, Loss: 0.1785\n",
      "Epoch 3, Batch 210, Loss: 0.1735\n",
      "Epoch 3, Batch 220, Loss: 0.1872\n",
      "Epoch 3, Batch 230, Loss: 0.2172\n",
      "Epoch 3, Batch 240, Loss: 0.1893\n",
      "Epoch 3, Batch 250, Loss: 0.1821\n",
      "Epoch 3, Batch 260, Loss: 0.1933\n",
      "Epoch 3, Batch 270, Loss: 0.2207\n",
      "Epoch 3, Batch 280, Loss: 0.1824\n",
      "Epoch 3, Batch 290, Loss: 0.1823\n",
      "Epoch 3, Batch 300, Loss: 0.1843\n",
      "Epoch 3, Batch 310, Loss: 0.1781\n",
      "Epoch 3, Batch 320, Loss: 0.1739\n",
      "Epoch 3, Batch 330, Loss: 0.1529\n",
      "Epoch 3, Batch 340, Loss: 0.1447\n",
      "Epoch 3, Batch 350, Loss: 0.1618\n",
      "Epoch 3, Batch 360, Loss: 0.2236\n",
      "Epoch 3, Batch 370, Loss: 0.2000\n",
      "Epoch 3, Batch 380, Loss: 0.1954\n",
      "Epoch 3, Batch 390, Loss: 0.2056\n",
      "Epoch 3, Batch 400, Loss: 0.2013\n",
      "Epoch 3, Batch 410, Loss: 0.1846\n",
      "Epoch 3, Batch 420, Loss: 0.1923\n",
      "Epoch 3, Batch 430, Loss: 0.1921\n",
      "Epoch 3, Batch 440, Loss: 0.2160\n",
      "Epoch 3, Batch 450, Loss: 0.2299\n",
      "Epoch 3, Batch 460, Loss: 0.2059\n",
      "Epoch 3, Batch 470, Loss: 0.1739\n",
      "Epoch 3, Batch 480, Loss: 0.1836\n",
      "Epoch 3, Batch 490, Loss: 0.1815\n",
      "Epoch 3, Batch 500, Loss: 0.1899\n",
      "Epoch 3, Batch 510, Loss: 0.1957\n",
      "Epoch 3, Batch 520, Loss: 0.1650\n",
      "Epoch 3, Batch 530, Loss: 0.1898\n",
      "Epoch 3, Batch 540, Loss: 0.1687\n",
      "Epoch 3, Batch 550, Loss: 0.2379\n",
      "Epoch 3, Batch 560, Loss: 0.1959\n",
      "Epoch 3, Batch 570, Loss: 0.1965\n",
      "Epoch 3, Batch 580, Loss: 0.1657\n",
      "Epoch 3, Batch 590, Loss: 0.1977\n",
      "Epoch 3, Batch 600, Loss: 0.1689\n",
      "Epoch 3, Batch 610, Loss: 0.2292\n",
      "Epoch 3, Batch 620, Loss: 0.1962\n",
      "Epoch 3, Batch 630, Loss: 0.1468\n",
      "Epoch 3, Batch 640, Loss: 0.1827\n",
      "Epoch 3, Batch 650, Loss: 0.1990\n",
      "Epoch 3, Batch 660, Loss: 0.1855\n",
      "Epoch 3, Batch 670, Loss: 0.2399\n",
      "Epoch 3, Batch 680, Loss: 0.1781\n",
      "Epoch 3, Batch 690, Loss: 0.2156\n",
      "Epoch 3, Batch 700, Loss: 0.2241\n",
      "Epoch 3, Batch 710, Loss: 0.1990\n",
      "Epoch 3, Batch 720, Loss: 0.1381\n",
      "Epoch 3, Batch 730, Loss: 0.1826\n",
      "Epoch 3, Batch 740, Loss: 0.2216\n",
      "Epoch 3, Batch 750, Loss: 0.1861\n",
      "Epoch 3, Batch 760, Loss: 0.1961\n",
      "Epoch 3, Batch 770, Loss: 0.1881\n",
      "Epoch 3, Batch 780, Loss: 0.1820\n",
      "Epoch 3, Batch 790, Loss: 0.1785\n",
      "Epoch 3, Batch 800, Loss: 0.1547\n",
      "Epoch 3, Batch 810, Loss: 0.1416\n",
      "Epoch 3, Batch 820, Loss: 0.1646\n",
      "Epoch 3, Batch 830, Loss: 0.1815\n",
      "Epoch 3, Batch 840, Loss: 0.1717\n",
      "Epoch 3, Batch 850, Loss: 0.1935\n",
      "Epoch 3, Batch 860, Loss: 0.1861\n",
      "Epoch 3, Batch 870, Loss: 0.1751\n",
      "Epoch 3, Batch 880, Loss: 0.1846\n",
      "Epoch 3, Batch 890, Loss: 0.1561\n",
      "Epoch 3, Batch 900, Loss: 0.2083\n",
      "Epoch 3, Batch 910, Loss: 0.2163\n",
      "Epoch 3, Batch 920, Loss: 0.2348\n",
      "Epoch 3, Batch 930, Loss: 0.1492\n",
      "Epoch 3, Batch 940, Loss: 0.2123\n",
      "Epoch 3, Batch 950, Loss: 0.1652\n",
      "Epoch 3, Batch 960, Loss: 0.1557\n",
      "Epoch 3, Batch 970, Loss: 0.2041\n",
      "Epoch 3, Batch 980, Loss: 0.2471\n",
      "Epoch 3, Batch 990, Loss: 0.1744\n",
      "Epoch 3, Batch 1000, Loss: 0.1757\n",
      "Epoch 3, Batch 1010, Loss: 0.2015\n",
      "Epoch 3, Batch 1020, Loss: 0.2031\n",
      "Epoch 3, Batch 1030, Loss: 0.1682\n",
      "Epoch 3, Batch 1040, Loss: 0.2194\n",
      "Epoch 3, Batch 1050, Loss: 0.2020\n",
      "Epoch 3, Batch 1060, Loss: 0.2067\n",
      "Epoch 3, Batch 1070, Loss: 0.1673\n",
      "Epoch 3, Batch 1080, Loss: 0.1735\n",
      "Epoch 3, Batch 1090, Loss: 0.1663\n",
      "Epoch 3, Batch 1100, Loss: 0.1791\n",
      "Epoch 3, Batch 1110, Loss: 0.2001\n",
      "Epoch 3, Batch 1120, Loss: 0.1690\n",
      "Epoch 3, Batch 1130, Loss: 0.1794\n",
      "Epoch 3, Batch 1140, Loss: 0.1773\n",
      "Epoch 3, Batch 1150, Loss: 0.1869\n",
      "Epoch 3, Batch 1160, Loss: 0.1855\n",
      "Epoch 3, Batch 1170, Loss: 0.1707\n",
      "Epoch 3, Batch 1180, Loss: 0.1665\n",
      "Epoch 3, Batch 1190, Loss: 0.1558\n",
      "Epoch 3, Batch 1200, Loss: 0.2137\n",
      "Epoch 3, Batch 1210, Loss: 0.1544\n",
      "Epoch 3, Batch 1220, Loss: 0.1865\n",
      "Epoch 3, Batch 1230, Loss: 0.1696\n",
      "Epoch 3, Batch 1240, Loss: 0.1614\n",
      "Epoch 3, Batch 1250, Loss: 0.1845\n",
      "Epoch 3, Batch 1260, Loss: 0.1868\n",
      "Epoch 3, Batch 1270, Loss: 0.2154\n",
      "Epoch 3, Batch 1280, Loss: 0.2066\n",
      "Epoch 3, Batch 1290, Loss: 0.2118\n",
      "Epoch 3, Batch 1300, Loss: 0.1906\n",
      "Epoch 3, Batch 1310, Loss: 0.1725\n",
      "Epoch 4, Batch 10, Loss: 0.1476\n",
      "Epoch 4, Batch 20, Loss: 0.1396\n",
      "Epoch 4, Batch 30, Loss: 0.1740\n",
      "Epoch 4, Batch 40, Loss: 0.1981\n",
      "Epoch 4, Batch 50, Loss: 0.1863\n",
      "Epoch 4, Batch 60, Loss: 0.2046\n",
      "Epoch 4, Batch 70, Loss: 0.2004\n",
      "Epoch 4, Batch 80, Loss: 0.1676\n",
      "Epoch 4, Batch 90, Loss: 0.1571\n",
      "Epoch 4, Batch 100, Loss: 0.1695\n",
      "Epoch 4, Batch 110, Loss: 0.1856\n",
      "Epoch 4, Batch 120, Loss: 0.1907\n",
      "Epoch 4, Batch 130, Loss: 0.1809\n",
      "Epoch 4, Batch 140, Loss: 0.1792\n",
      "Epoch 4, Batch 150, Loss: 0.1948\n",
      "Epoch 4, Batch 160, Loss: 0.1759\n",
      "Epoch 4, Batch 170, Loss: 0.2049\n",
      "Epoch 4, Batch 180, Loss: 0.1732\n",
      "Epoch 4, Batch 190, Loss: 0.1790\n",
      "Epoch 4, Batch 200, Loss: 0.1377\n",
      "Epoch 4, Batch 210, Loss: 0.1829\n",
      "Epoch 4, Batch 220, Loss: 0.1823\n",
      "Epoch 4, Batch 230, Loss: 0.1629\n",
      "Epoch 4, Batch 240, Loss: 0.1748\n",
      "Epoch 4, Batch 250, Loss: 0.2049\n",
      "Epoch 4, Batch 260, Loss: 0.1711\n",
      "Epoch 4, Batch 270, Loss: 0.1701\n",
      "Epoch 4, Batch 280, Loss: 0.1559\n",
      "Epoch 4, Batch 290, Loss: 0.1606\n",
      "Epoch 4, Batch 300, Loss: 0.1575\n",
      "Epoch 4, Batch 310, Loss: 0.1684\n",
      "Epoch 4, Batch 320, Loss: 0.1296\n",
      "Epoch 4, Batch 330, Loss: 0.1388\n",
      "Epoch 4, Batch 340, Loss: 0.1490\n",
      "Epoch 4, Batch 350, Loss: 0.1883\n",
      "Epoch 4, Batch 360, Loss: 0.1853\n",
      "Epoch 4, Batch 370, Loss: 0.1854\n",
      "Epoch 4, Batch 380, Loss: 0.2236\n",
      "Epoch 4, Batch 390, Loss: 0.1863\n",
      "Epoch 4, Batch 400, Loss: 0.1420\n",
      "Epoch 4, Batch 410, Loss: 0.1480\n",
      "Epoch 4, Batch 420, Loss: 0.1832\n",
      "Epoch 4, Batch 430, Loss: 0.1319\n",
      "Epoch 4, Batch 440, Loss: 0.2184\n",
      "Epoch 4, Batch 450, Loss: 0.1567\n",
      "Epoch 4, Batch 460, Loss: 0.2210\n",
      "Epoch 4, Batch 470, Loss: 0.1957\n",
      "Epoch 4, Batch 480, Loss: 0.1803\n",
      "Epoch 4, Batch 490, Loss: 0.1758\n",
      "Epoch 4, Batch 500, Loss: 0.2239\n",
      "Epoch 4, Batch 510, Loss: 0.1838\n",
      "Epoch 4, Batch 520, Loss: 0.1472\n",
      "Epoch 4, Batch 530, Loss: 0.1651\n",
      "Epoch 4, Batch 540, Loss: 0.1530\n",
      "Epoch 4, Batch 550, Loss: 0.1500\n",
      "Epoch 4, Batch 560, Loss: 0.1796\n",
      "Epoch 4, Batch 570, Loss: 0.1592\n",
      "Epoch 4, Batch 580, Loss: 0.1499\n",
      "Epoch 4, Batch 590, Loss: 0.2217\n",
      "Epoch 4, Batch 600, Loss: 0.1588\n",
      "Epoch 4, Batch 610, Loss: 0.1454\n",
      "Epoch 4, Batch 620, Loss: 0.1738\n",
      "Epoch 4, Batch 630, Loss: 0.2055\n",
      "Epoch 4, Batch 640, Loss: 0.1861\n",
      "Epoch 4, Batch 650, Loss: 0.2317\n",
      "Epoch 4, Batch 660, Loss: 0.2024\n",
      "Epoch 4, Batch 670, Loss: 0.1768\n",
      "Epoch 4, Batch 680, Loss: 0.1522\n",
      "Epoch 4, Batch 690, Loss: 0.1582\n",
      "Epoch 4, Batch 700, Loss: 0.1578\n",
      "Epoch 4, Batch 710, Loss: 0.1511\n",
      "Epoch 4, Batch 720, Loss: 0.1477\n",
      "Epoch 4, Batch 730, Loss: 0.1402\n",
      "Epoch 4, Batch 740, Loss: 0.1676\n",
      "Epoch 4, Batch 750, Loss: 0.2144\n",
      "Epoch 4, Batch 760, Loss: 0.2175\n",
      "Epoch 4, Batch 770, Loss: 0.1955\n",
      "Epoch 4, Batch 780, Loss: 0.1881\n",
      "Epoch 4, Batch 790, Loss: 0.1679\n",
      "Epoch 4, Batch 800, Loss: 0.1678\n",
      "Epoch 4, Batch 810, Loss: 0.2042\n",
      "Epoch 4, Batch 820, Loss: 0.1793\n",
      "Epoch 4, Batch 830, Loss: 0.1494\n",
      "Epoch 4, Batch 840, Loss: 0.1681\n",
      "Epoch 4, Batch 850, Loss: 0.1973\n",
      "Epoch 4, Batch 860, Loss: 0.1436\n",
      "Epoch 4, Batch 870, Loss: 0.1759\n",
      "Epoch 4, Batch 880, Loss: 0.2222\n",
      "Epoch 4, Batch 890, Loss: 0.2026\n",
      "Epoch 4, Batch 900, Loss: 0.1119\n",
      "Epoch 4, Batch 910, Loss: 0.1704\n",
      "Epoch 4, Batch 920, Loss: 0.2107\n",
      "Epoch 4, Batch 930, Loss: 0.1915\n",
      "Epoch 4, Batch 940, Loss: 0.1684\n",
      "Epoch 4, Batch 950, Loss: 0.1383\n",
      "Epoch 4, Batch 960, Loss: 0.1679\n",
      "Epoch 4, Batch 970, Loss: 0.1500\n",
      "Epoch 4, Batch 980, Loss: 0.1820\n",
      "Epoch 4, Batch 990, Loss: 0.1735\n",
      "Epoch 4, Batch 1000, Loss: 0.1703\n",
      "Epoch 4, Batch 1010, Loss: 0.1520\n",
      "Epoch 4, Batch 1020, Loss: 0.1487\n",
      "Epoch 4, Batch 1030, Loss: 0.1431\n",
      "Epoch 4, Batch 1040, Loss: 0.1160\n",
      "Epoch 4, Batch 1050, Loss: 0.1704\n",
      "Epoch 4, Batch 1060, Loss: 0.1622\n",
      "Epoch 4, Batch 1070, Loss: 0.1595\n",
      "Epoch 4, Batch 1080, Loss: 0.2204\n",
      "Epoch 4, Batch 1090, Loss: 0.1919\n",
      "Epoch 4, Batch 1100, Loss: 0.1762\n",
      "Epoch 4, Batch 1110, Loss: 0.1707\n",
      "Epoch 4, Batch 1120, Loss: 0.1628\n",
      "Epoch 4, Batch 1130, Loss: 0.1869\n",
      "Epoch 4, Batch 1140, Loss: 0.1979\n",
      "Epoch 4, Batch 1150, Loss: 0.1616\n",
      "Epoch 4, Batch 1160, Loss: 0.1704\n",
      "Epoch 4, Batch 1170, Loss: 0.1726\n",
      "Epoch 4, Batch 1180, Loss: 0.1559\n",
      "Epoch 4, Batch 1190, Loss: 0.1391\n",
      "Epoch 4, Batch 1200, Loss: 0.1681\n",
      "Epoch 4, Batch 1210, Loss: 0.1624\n",
      "Epoch 4, Batch 1220, Loss: 0.1952\n",
      "Epoch 4, Batch 1230, Loss: 0.1818\n",
      "Epoch 4, Batch 1240, Loss: 0.1879\n",
      "Epoch 4, Batch 1250, Loss: 0.1968\n",
      "Epoch 4, Batch 1260, Loss: 0.1250\n",
      "Epoch 4, Batch 1270, Loss: 0.1325\n",
      "Epoch 4, Batch 1280, Loss: 0.1646\n",
      "Epoch 4, Batch 1290, Loss: 0.1667\n",
      "Epoch 4, Batch 1300, Loss: 0.1616\n",
      "Epoch 4, Batch 1310, Loss: 0.1972\n",
      "Epoch 5, Batch 10, Loss: 0.1677\n",
      "Epoch 5, Batch 20, Loss: 0.1519\n",
      "Epoch 5, Batch 30, Loss: 0.1545\n",
      "Epoch 5, Batch 40, Loss: 0.1673\n",
      "Epoch 5, Batch 50, Loss: 0.1725\n",
      "Epoch 5, Batch 60, Loss: 0.1637\n",
      "Epoch 5, Batch 70, Loss: 0.1476\n",
      "Epoch 5, Batch 80, Loss: 0.1748\n",
      "Epoch 5, Batch 90, Loss: 0.1890\n",
      "Epoch 5, Batch 100, Loss: 0.1639\n",
      "Epoch 5, Batch 110, Loss: 0.1388\n",
      "Epoch 5, Batch 120, Loss: 0.1516\n",
      "Epoch 5, Batch 130, Loss: 0.1807\n",
      "Epoch 5, Batch 140, Loss: 0.1801\n",
      "Epoch 5, Batch 150, Loss: 0.1589\n",
      "Epoch 5, Batch 160, Loss: 0.1615\n",
      "Epoch 5, Batch 170, Loss: 0.1670\n",
      "Epoch 5, Batch 180, Loss: 0.1600\n",
      "Epoch 5, Batch 190, Loss: 0.1400\n",
      "Epoch 5, Batch 200, Loss: 0.1947\n",
      "Epoch 5, Batch 210, Loss: 0.1450\n",
      "Epoch 5, Batch 220, Loss: 0.1643\n",
      "Epoch 5, Batch 230, Loss: 0.1464\n",
      "Epoch 5, Batch 240, Loss: 0.1690\n",
      "Epoch 5, Batch 250, Loss: 0.1731\n",
      "Epoch 5, Batch 260, Loss: 0.1741\n",
      "Epoch 5, Batch 270, Loss: 0.1602\n",
      "Epoch 5, Batch 280, Loss: 0.1890\n",
      "Epoch 5, Batch 290, Loss: 0.1451\n",
      "Epoch 5, Batch 300, Loss: 0.1458\n",
      "Epoch 5, Batch 310, Loss: 0.1592\n",
      "Epoch 5, Batch 320, Loss: 0.1512\n",
      "Epoch 5, Batch 330, Loss: 0.1665\n",
      "Epoch 5, Batch 340, Loss: 0.1584\n",
      "Epoch 5, Batch 350, Loss: 0.1853\n",
      "Epoch 5, Batch 360, Loss: 0.1657\n",
      "Epoch 5, Batch 370, Loss: 0.1729\n",
      "Epoch 5, Batch 380, Loss: 0.1941\n",
      "Epoch 5, Batch 390, Loss: 0.1395\n",
      "Epoch 5, Batch 400, Loss: 0.1473\n",
      "Epoch 5, Batch 410, Loss: 0.1351\n",
      "Epoch 5, Batch 420, Loss: 0.1893\n",
      "Epoch 5, Batch 430, Loss: 0.2106\n",
      "Epoch 5, Batch 440, Loss: 0.2000\n",
      "Epoch 5, Batch 450, Loss: 0.1789\n",
      "Epoch 5, Batch 460, Loss: 0.1471\n",
      "Epoch 5, Batch 470, Loss: 0.1868\n",
      "Epoch 5, Batch 480, Loss: 0.1679\n",
      "Epoch 5, Batch 490, Loss: 0.1410\n",
      "Epoch 5, Batch 500, Loss: 0.1607\n",
      "Epoch 5, Batch 510, Loss: 0.1100\n",
      "Epoch 5, Batch 520, Loss: 0.1434\n",
      "Epoch 5, Batch 530, Loss: 0.1408\n",
      "Epoch 5, Batch 540, Loss: 0.1334\n",
      "Epoch 5, Batch 550, Loss: 0.1473\n",
      "Epoch 5, Batch 560, Loss: 0.1578\n",
      "Epoch 5, Batch 570, Loss: 0.1716\n",
      "Epoch 5, Batch 580, Loss: 0.1421\n",
      "Epoch 5, Batch 590, Loss: 0.1782\n",
      "Epoch 5, Batch 600, Loss: 0.1249\n",
      "Epoch 5, Batch 610, Loss: 0.1280\n",
      "Epoch 5, Batch 620, Loss: 0.1655\n",
      "Epoch 5, Batch 630, Loss: 0.1714\n",
      "Epoch 5, Batch 640, Loss: 0.1584\n",
      "Epoch 5, Batch 650, Loss: 0.1515\n",
      "Epoch 5, Batch 660, Loss: 0.1150\n",
      "Epoch 5, Batch 670, Loss: 0.1417\n",
      "Epoch 5, Batch 680, Loss: 0.1456\n",
      "Epoch 5, Batch 690, Loss: 0.1646\n",
      "Epoch 5, Batch 700, Loss: 0.1516\n",
      "Epoch 5, Batch 710, Loss: 0.1871\n",
      "Epoch 5, Batch 720, Loss: 0.1581\n",
      "Epoch 5, Batch 730, Loss: 0.1783\n",
      "Epoch 5, Batch 740, Loss: 0.1582\n",
      "Epoch 5, Batch 750, Loss: 0.1394\n",
      "Epoch 5, Batch 760, Loss: 0.1581\n",
      "Epoch 5, Batch 770, Loss: 0.1276\n",
      "Epoch 5, Batch 780, Loss: 0.1555\n",
      "Epoch 5, Batch 790, Loss: 0.1409\n",
      "Epoch 5, Batch 800, Loss: 0.1748\n",
      "Epoch 5, Batch 810, Loss: 0.2018\n",
      "Epoch 5, Batch 820, Loss: 0.1817\n",
      "Epoch 5, Batch 830, Loss: 0.1917\n",
      "Epoch 5, Batch 840, Loss: 0.1550\n",
      "Epoch 5, Batch 850, Loss: 0.1618\n",
      "Epoch 5, Batch 860, Loss: 0.1416\n",
      "Epoch 5, Batch 870, Loss: 0.1517\n",
      "Epoch 5, Batch 880, Loss: 0.1772\n",
      "Epoch 5, Batch 890, Loss: 0.1361\n",
      "Epoch 5, Batch 900, Loss: 0.1487\n",
      "Epoch 5, Batch 910, Loss: 0.1339\n",
      "Epoch 5, Batch 920, Loss: 0.1366\n",
      "Epoch 5, Batch 930, Loss: 0.1531\n",
      "Epoch 5, Batch 940, Loss: 0.1680\n",
      "Epoch 5, Batch 950, Loss: 0.1410\n",
      "Epoch 5, Batch 960, Loss: 0.1567\n",
      "Epoch 5, Batch 970, Loss: 0.1540\n",
      "Epoch 5, Batch 980, Loss: 0.2236\n",
      "Epoch 5, Batch 990, Loss: 0.1379\n",
      "Epoch 5, Batch 1000, Loss: 0.1862\n",
      "Epoch 5, Batch 1010, Loss: 0.1800\n",
      "Epoch 5, Batch 1020, Loss: 0.1900\n",
      "Epoch 5, Batch 1030, Loss: 0.1657\n",
      "Epoch 5, Batch 1040, Loss: 0.1629\n",
      "Epoch 5, Batch 1050, Loss: 0.2075\n",
      "Epoch 5, Batch 1060, Loss: 0.1805\n",
      "Epoch 5, Batch 1070, Loss: 0.1519\n",
      "Epoch 5, Batch 1080, Loss: 0.1505\n",
      "Epoch 5, Batch 1090, Loss: 0.1511\n",
      "Epoch 5, Batch 1100, Loss: 0.1348\n",
      "Epoch 5, Batch 1110, Loss: 0.1670\n",
      "Epoch 5, Batch 1120, Loss: 0.1373\n",
      "Epoch 5, Batch 1130, Loss: 0.1832\n",
      "Epoch 5, Batch 1140, Loss: 0.1449\n",
      "Epoch 5, Batch 1150, Loss: 0.1701\n",
      "Epoch 5, Batch 1160, Loss: 0.1257\n",
      "Epoch 5, Batch 1170, Loss: 0.1423\n",
      "Epoch 5, Batch 1180, Loss: 0.1587\n",
      "Epoch 5, Batch 1190, Loss: 0.1434\n",
      "Epoch 5, Batch 1200, Loss: 0.1569\n",
      "Epoch 5, Batch 1210, Loss: 0.1145\n",
      "Epoch 5, Batch 1220, Loss: 0.1656\n",
      "Epoch 5, Batch 1230, Loss: 0.1876\n",
      "Epoch 5, Batch 1240, Loss: 0.1548\n",
      "Epoch 5, Batch 1250, Loss: 0.1296\n",
      "Epoch 5, Batch 1260, Loss: 0.1373\n",
      "Epoch 5, Batch 1270, Loss: 0.1386\n",
      "Epoch 5, Batch 1280, Loss: 0.1265\n",
      "Epoch 5, Batch 1290, Loss: 0.1545\n",
      "Epoch 5, Batch 1300, Loss: 0.1461\n",
      "Epoch 5, Batch 1310, Loss: 0.1347\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 93.72%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "# Create the training and testing splits\n",
    "train_size = int(0.7 * len(data_set))\n",
    "test_size = len(data_set) - train_size\n",
    "train_dataset, test_dataset = random_split(data_set, [train_size, test_size])\n",
    "\n",
    "# Dataloader for batch training\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()  # BCELoss expects labels to be in float format\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()  # Remove unnecessary dimensions\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # Print every 10 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_model(model, train_loader, criterion, optimizer, epochs)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee99d4e9-acfe-44d3-88b2-90a5b30f75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'paper_model_cifake.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04780d5a-4dae-47ac-8dcb-6686ecd97b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 93.72%\n",
      "Precision of the network on the test images: 91.74%\n",
      "Recall of the network on the test images: 96.10%\n",
      "Specificity of the network on the test images: 95.90%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Test the model and calculate precision, recall, and specificity\n",
    "def test_model_with_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = 100 * precision_score(all_labels, all_predictions)\n",
    "    recall = 100 * recall_score(all_labels, all_predictions)\n",
    "\n",
    "    # Calculate specificity\n",
    "    TN = ((1 - np.array(all_predictions)) * (1 - np.array(all_labels))).sum()\n",
    "    FN = ((1 - np.array(all_predictions)) * np.array(all_labels)).sum()\n",
    "    specificity = 100 * TN / (TN + FN)\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    print(f'Precision of the network on the test images: {precision:.2f}%')\n",
    "    print(f'Recall of the network on the test images: {recall:.2f}%')\n",
    "    print(f'Specificity of the network on the test images: {specificity:.2f}%')\n",
    "\n",
    "# Call the modified test function\n",
    "test_model_with_metrics(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f160bffe-efdd-4767-a132-7288eeffffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image):\n",
    "    \"\"\"Transform the image to the required input size and format.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def evaluate_folder(model, folder_path):\n",
    "    \"\"\"Evaluate a single folder of images and return the counts and rates.\"\"\"\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        try:\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform_image(image)\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "        except:\n",
    "            print(f\"Warning: failed to load one image {image_path}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(output)  # Round to get binary predictions\n",
    "            label = predicted.item()\n",
    "\n",
    "        if label == 0:\n",
    "            count_0 += 1\n",
    "        else:\n",
    "            count_1 += 1\n",
    "\n",
    "        total_images += 1\n",
    "\n",
    "    rate_0 = (count_0 / total_images) * 100 if total_images > 0 else 0\n",
    "    rate_1 = (count_1 / total_images) * 100 if total_images > 0 else 0\n",
    "\n",
    "    return count_0, count_1, rate_0, rate_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20807563-3869-456a-9d25-8ebcea2e512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 27, 86.5, 13.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/sd_2_1_frogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6375dc26-7faf-4919-adb6-5528b9b9072e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 121, 39.5, 60.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/sd_2_1_cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5325872-4c93-4f74-9f7d-8dc48a709172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 93, 53.5, 46.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/sd_2_1_dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095a56c1-cf00-4540-902e-037f6712033c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 16, 75.0, 25.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/sd_2_1_dogs_with_modifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1c4079-d6af-4d05-8d11-c2d8ff4d3d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 31, 84.5, 15.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/sd_2_1_airplanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "072e95c8-6f27-4a70-b9d1-ff8136220b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/web-airplanes\\source.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 112, 40.74074074074074, 59.25925925925925)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/web-airplanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "176b7885-fc08-414b-bd71-95837f58273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/Cat\\source.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(309, 12190, 2.4721977758220657, 97.52780222417795)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9347196a-b857-4a63-b44e-be652c77ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\licaili193\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(275, 12224, 2.2001760140811264, 97.79982398591888)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/Dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70f13849-e7b7-4362-a233-3706dc9aa478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 9629, 3.71, 96.28999999999999)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/CIFAKE/test/REAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d13be57-0740-4aa9-9c7c-5947e58b73e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9203, 797, 92.03, 7.969999999999999)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model, \"data/CIFAKE/test/FAKE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6f44c2-3196-4fa6-98b7-2f4939eaf41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset 2...\n",
      "Loading folder: data/CIFAKE-2\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af99420ca49b447db13e84ff880971ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-2\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e198020858684f8c8f954244160900bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-2\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cb788978454b04a97ee07b5d75a02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-2\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f96263070474699a3ce63e2992ca341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 49998\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path_2 = \"data/CIFAKE-2\"\n",
    "print(\"Loading CIFAKEDataset 2...\")\n",
    "data_set_2 = CIFAKEDataset(folder_path_2, num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(data_set_2))\n",
    "print(\"Data dimension:\", data_set_2.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "data_set_2.show_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f7205b-755c-434c-a3dc-28f166686b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAKEClassifier(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model_2 = CIFAKEClassifier()\n",
    "\n",
    "# Print the model structure\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf8a0e6a-6a6f-44c6-a938-619f934b0f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 0.6761\n",
      "Epoch 1, Batch 20, Loss: 0.6546\n",
      "Epoch 1, Batch 30, Loss: 0.5922\n",
      "Epoch 1, Batch 40, Loss: 0.5229\n",
      "Epoch 1, Batch 50, Loss: 0.4872\n",
      "Epoch 1, Batch 60, Loss: 0.4279\n",
      "Epoch 1, Batch 70, Loss: 0.3998\n",
      "Epoch 1, Batch 80, Loss: 0.3053\n",
      "Epoch 1, Batch 90, Loss: 0.3117\n",
      "Epoch 1, Batch 100, Loss: 0.2949\n",
      "Epoch 1, Batch 110, Loss: 0.3250\n",
      "Epoch 1, Batch 120, Loss: 0.2941\n",
      "Epoch 1, Batch 130, Loss: 0.2574\n",
      "Epoch 1, Batch 140, Loss: 0.2771\n",
      "Epoch 1, Batch 150, Loss: 0.2564\n",
      "Epoch 1, Batch 160, Loss: 0.2963\n",
      "Epoch 1, Batch 170, Loss: 0.2698\n",
      "Epoch 1, Batch 180, Loss: 0.2563\n",
      "Epoch 1, Batch 190, Loss: 0.2404\n",
      "Epoch 1, Batch 200, Loss: 0.2785\n",
      "Epoch 1, Batch 210, Loss: 0.2769\n",
      "Epoch 1, Batch 220, Loss: 0.2440\n",
      "Epoch 1, Batch 230, Loss: 0.2136\n",
      "Epoch 1, Batch 240, Loss: 0.2456\n",
      "Epoch 1, Batch 250, Loss: 0.2840\n",
      "Epoch 1, Batch 260, Loss: 0.2837\n",
      "Epoch 1, Batch 270, Loss: 0.2821\n",
      "Epoch 1, Batch 280, Loss: 0.2279\n",
      "Epoch 1, Batch 290, Loss: 0.2240\n",
      "Epoch 1, Batch 300, Loss: 0.2111\n",
      "Epoch 1, Batch 310, Loss: 0.2230\n",
      "Epoch 1, Batch 320, Loss: 0.2345\n",
      "Epoch 1, Batch 330, Loss: 0.2336\n",
      "Epoch 1, Batch 340, Loss: 0.2497\n",
      "Epoch 1, Batch 350, Loss: 0.2236\n",
      "Epoch 1, Batch 360, Loss: 0.1831\n",
      "Epoch 1, Batch 370, Loss: 0.2425\n",
      "Epoch 1, Batch 380, Loss: 0.2237\n",
      "Epoch 1, Batch 390, Loss: 0.2207\n",
      "Epoch 1, Batch 400, Loss: 0.2056\n",
      "Epoch 1, Batch 410, Loss: 0.2075\n",
      "Epoch 1, Batch 420, Loss: 0.2022\n",
      "Epoch 1, Batch 430, Loss: 0.1903\n",
      "Epoch 1, Batch 440, Loss: 0.2386\n",
      "Epoch 1, Batch 450, Loss: 0.2242\n",
      "Epoch 1, Batch 460, Loss: 0.2072\n",
      "Epoch 1, Batch 470, Loss: 0.2240\n",
      "Epoch 1, Batch 480, Loss: 0.2072\n",
      "Epoch 1, Batch 490, Loss: 0.2079\n",
      "Epoch 1, Batch 500, Loss: 0.1997\n",
      "Epoch 1, Batch 510, Loss: 0.1910\n",
      "Epoch 1, Batch 520, Loss: 0.2096\n",
      "Epoch 1, Batch 530, Loss: 0.2031\n",
      "Epoch 1, Batch 540, Loss: 0.1555\n",
      "Epoch 2, Batch 10, Loss: 0.1800\n",
      "Epoch 2, Batch 20, Loss: 0.2371\n",
      "Epoch 2, Batch 30, Loss: 0.1796\n",
      "Epoch 2, Batch 40, Loss: 0.2081\n",
      "Epoch 2, Batch 50, Loss: 0.1758\n",
      "Epoch 2, Batch 60, Loss: 0.1604\n",
      "Epoch 2, Batch 70, Loss: 0.1562\n",
      "Epoch 2, Batch 80, Loss: 0.2061\n",
      "Epoch 2, Batch 90, Loss: 0.1768\n",
      "Epoch 2, Batch 100, Loss: 0.2499\n",
      "Epoch 2, Batch 110, Loss: 0.1962\n",
      "Epoch 2, Batch 120, Loss: 0.1687\n",
      "Epoch 2, Batch 130, Loss: 0.1827\n",
      "Epoch 2, Batch 140, Loss: 0.1692\n",
      "Epoch 2, Batch 150, Loss: 0.1539\n",
      "Epoch 2, Batch 160, Loss: 0.1893\n",
      "Epoch 2, Batch 170, Loss: 0.2045\n",
      "Epoch 2, Batch 180, Loss: 0.1796\n",
      "Epoch 2, Batch 190, Loss: 0.1334\n",
      "Epoch 2, Batch 200, Loss: 0.1478\n",
      "Epoch 2, Batch 210, Loss: 0.1632\n",
      "Epoch 2, Batch 220, Loss: 0.1734\n",
      "Epoch 2, Batch 230, Loss: 0.1425\n",
      "Epoch 2, Batch 240, Loss: 0.1568\n",
      "Epoch 2, Batch 250, Loss: 0.1560\n",
      "Epoch 2, Batch 260, Loss: 0.1483\n",
      "Epoch 2, Batch 270, Loss: 0.1688\n",
      "Epoch 2, Batch 280, Loss: 0.1533\n",
      "Epoch 2, Batch 290, Loss: 0.1220\n",
      "Epoch 2, Batch 300, Loss: 0.1696\n",
      "Epoch 2, Batch 310, Loss: 0.1713\n",
      "Epoch 2, Batch 320, Loss: 0.1450\n",
      "Epoch 2, Batch 330, Loss: 0.1761\n",
      "Epoch 2, Batch 340, Loss: 0.1610\n",
      "Epoch 2, Batch 350, Loss: 0.2015\n",
      "Epoch 2, Batch 360, Loss: 0.1887\n",
      "Epoch 2, Batch 370, Loss: 0.2072\n",
      "Epoch 2, Batch 380, Loss: 0.1432\n",
      "Epoch 2, Batch 390, Loss: 0.1552\n",
      "Epoch 2, Batch 400, Loss: 0.2109\n",
      "Epoch 2, Batch 410, Loss: 0.1180\n",
      "Epoch 2, Batch 420, Loss: 0.1369\n",
      "Epoch 2, Batch 430, Loss: 0.1576\n",
      "Epoch 2, Batch 440, Loss: 0.1494\n",
      "Epoch 2, Batch 450, Loss: 0.1268\n",
      "Epoch 2, Batch 460, Loss: 0.1397\n",
      "Epoch 2, Batch 470, Loss: 0.1664\n",
      "Epoch 2, Batch 480, Loss: 0.1604\n",
      "Epoch 2, Batch 490, Loss: 0.1147\n",
      "Epoch 2, Batch 500, Loss: 0.1716\n",
      "Epoch 2, Batch 510, Loss: 0.1902\n",
      "Epoch 2, Batch 520, Loss: 0.1997\n",
      "Epoch 2, Batch 530, Loss: 0.1612\n",
      "Epoch 2, Batch 540, Loss: 0.1826\n",
      "Epoch 3, Batch 10, Loss: 0.2711\n",
      "Epoch 3, Batch 20, Loss: 0.1636\n",
      "Epoch 3, Batch 30, Loss: 0.1554\n",
      "Epoch 3, Batch 40, Loss: 0.1255\n",
      "Epoch 3, Batch 50, Loss: 0.1706\n",
      "Epoch 3, Batch 60, Loss: 0.1643\n",
      "Epoch 3, Batch 70, Loss: 0.1346\n",
      "Epoch 3, Batch 80, Loss: 0.1323\n",
      "Epoch 3, Batch 90, Loss: 0.1624\n",
      "Epoch 3, Batch 100, Loss: 0.1562\n",
      "Epoch 3, Batch 110, Loss: 0.1950\n",
      "Epoch 3, Batch 120, Loss: 0.1455\n",
      "Epoch 3, Batch 130, Loss: 0.1718\n",
      "Epoch 3, Batch 140, Loss: 0.1470\n",
      "Epoch 3, Batch 150, Loss: 0.1827\n",
      "Epoch 3, Batch 160, Loss: 0.1498\n",
      "Epoch 3, Batch 170, Loss: 0.1479\n",
      "Epoch 3, Batch 180, Loss: 0.1426\n",
      "Epoch 3, Batch 190, Loss: 0.1675\n",
      "Epoch 3, Batch 200, Loss: 0.1440\n",
      "Epoch 3, Batch 210, Loss: 0.1543\n",
      "Epoch 3, Batch 220, Loss: 0.1819\n",
      "Epoch 3, Batch 230, Loss: 0.1632\n",
      "Epoch 3, Batch 240, Loss: 0.1147\n",
      "Epoch 3, Batch 250, Loss: 0.1447\n",
      "Epoch 3, Batch 260, Loss: 0.1007\n",
      "Epoch 3, Batch 270, Loss: 0.1272\n",
      "Epoch 3, Batch 280, Loss: 0.1223\n",
      "Epoch 3, Batch 290, Loss: 0.1221\n",
      "Epoch 3, Batch 300, Loss: 0.1401\n",
      "Epoch 3, Batch 310, Loss: 0.1247\n",
      "Epoch 3, Batch 320, Loss: 0.1681\n",
      "Epoch 3, Batch 330, Loss: 0.1388\n",
      "Epoch 3, Batch 340, Loss: 0.1277\n",
      "Epoch 3, Batch 350, Loss: 0.1526\n",
      "Epoch 3, Batch 360, Loss: 0.1231\n",
      "Epoch 3, Batch 370, Loss: 0.1407\n",
      "Epoch 3, Batch 380, Loss: 0.1484\n",
      "Epoch 3, Batch 390, Loss: 0.1125\n",
      "Epoch 3, Batch 400, Loss: 0.1389\n",
      "Epoch 3, Batch 410, Loss: 0.1219\n",
      "Epoch 3, Batch 420, Loss: 0.1499\n",
      "Epoch 3, Batch 430, Loss: 0.1119\n",
      "Epoch 3, Batch 440, Loss: 0.1362\n",
      "Epoch 3, Batch 450, Loss: 0.1278\n",
      "Epoch 3, Batch 460, Loss: 0.0972\n",
      "Epoch 3, Batch 470, Loss: 0.1472\n",
      "Epoch 3, Batch 480, Loss: 0.1418\n",
      "Epoch 3, Batch 490, Loss: 0.1540\n",
      "Epoch 3, Batch 500, Loss: 0.1592\n",
      "Epoch 3, Batch 510, Loss: 0.1418\n",
      "Epoch 3, Batch 520, Loss: 0.1106\n",
      "Epoch 3, Batch 530, Loss: 0.1176\n",
      "Epoch 3, Batch 540, Loss: 0.1594\n",
      "Epoch 4, Batch 10, Loss: 0.1192\n",
      "Epoch 4, Batch 20, Loss: 0.1295\n",
      "Epoch 4, Batch 30, Loss: 0.1289\n",
      "Epoch 4, Batch 40, Loss: 0.1667\n",
      "Epoch 4, Batch 50, Loss: 0.1252\n",
      "Epoch 4, Batch 60, Loss: 0.1256\n",
      "Epoch 4, Batch 70, Loss: 0.1296\n",
      "Epoch 4, Batch 80, Loss: 0.1439\n",
      "Epoch 4, Batch 90, Loss: 0.1543\n",
      "Epoch 4, Batch 100, Loss: 0.1535\n",
      "Epoch 4, Batch 110, Loss: 0.1188\n",
      "Epoch 4, Batch 120, Loss: 0.1353\n",
      "Epoch 4, Batch 130, Loss: 0.1007\n",
      "Epoch 4, Batch 140, Loss: 0.1098\n",
      "Epoch 4, Batch 150, Loss: 0.0942\n",
      "Epoch 4, Batch 160, Loss: 0.1188\n",
      "Epoch 4, Batch 170, Loss: 0.1263\n",
      "Epoch 4, Batch 180, Loss: 0.0999\n",
      "Epoch 4, Batch 190, Loss: 0.1001\n",
      "Epoch 4, Batch 200, Loss: 0.1365\n",
      "Epoch 4, Batch 210, Loss: 0.1176\n",
      "Epoch 4, Batch 220, Loss: 0.1444\n",
      "Epoch 4, Batch 230, Loss: 0.1450\n",
      "Epoch 4, Batch 240, Loss: 0.1439\n",
      "Epoch 4, Batch 250, Loss: 0.1365\n",
      "Epoch 4, Batch 260, Loss: 0.1333\n",
      "Epoch 4, Batch 270, Loss: 0.1193\n",
      "Epoch 4, Batch 280, Loss: 0.1218\n",
      "Epoch 4, Batch 290, Loss: 0.1166\n",
      "Epoch 4, Batch 300, Loss: 0.1224\n",
      "Epoch 4, Batch 310, Loss: 0.1216\n",
      "Epoch 4, Batch 320, Loss: 0.1140\n",
      "Epoch 4, Batch 330, Loss: 0.1071\n",
      "Epoch 4, Batch 340, Loss: 0.0992\n",
      "Epoch 4, Batch 350, Loss: 0.1129\n",
      "Epoch 4, Batch 360, Loss: 0.1340\n",
      "Epoch 4, Batch 370, Loss: 0.1085\n",
      "Epoch 4, Batch 380, Loss: 0.0964\n",
      "Epoch 4, Batch 390, Loss: 0.1197\n",
      "Epoch 4, Batch 400, Loss: 0.1017\n",
      "Epoch 4, Batch 410, Loss: 0.0924\n",
      "Epoch 4, Batch 420, Loss: 0.1297\n",
      "Epoch 4, Batch 430, Loss: 0.1323\n",
      "Epoch 4, Batch 440, Loss: 0.1061\n",
      "Epoch 4, Batch 450, Loss: 0.1140\n",
      "Epoch 4, Batch 460, Loss: 0.1015\n",
      "Epoch 4, Batch 470, Loss: 0.1121\n",
      "Epoch 4, Batch 480, Loss: 0.1091\n",
      "Epoch 4, Batch 490, Loss: 0.0869\n",
      "Epoch 4, Batch 500, Loss: 0.0978\n",
      "Epoch 4, Batch 510, Loss: 0.1508\n",
      "Epoch 4, Batch 520, Loss: 0.0821\n",
      "Epoch 4, Batch 530, Loss: 0.1194\n",
      "Epoch 4, Batch 540, Loss: 0.1052\n",
      "Epoch 5, Batch 10, Loss: 0.0773\n",
      "Epoch 5, Batch 20, Loss: 0.1154\n",
      "Epoch 5, Batch 30, Loss: 0.1176\n",
      "Epoch 5, Batch 40, Loss: 0.0933\n",
      "Epoch 5, Batch 50, Loss: 0.0959\n",
      "Epoch 5, Batch 60, Loss: 0.1257\n",
      "Epoch 5, Batch 70, Loss: 0.1197\n",
      "Epoch 5, Batch 80, Loss: 0.0974\n",
      "Epoch 5, Batch 90, Loss: 0.0808\n",
      "Epoch 5, Batch 100, Loss: 0.1138\n",
      "Epoch 5, Batch 110, Loss: 0.1026\n",
      "Epoch 5, Batch 120, Loss: 0.1091\n",
      "Epoch 5, Batch 130, Loss: 0.1001\n",
      "Epoch 5, Batch 140, Loss: 0.0917\n",
      "Epoch 5, Batch 150, Loss: 0.0753\n",
      "Epoch 5, Batch 160, Loss: 0.0991\n",
      "Epoch 5, Batch 170, Loss: 0.1072\n",
      "Epoch 5, Batch 180, Loss: 0.0984\n",
      "Epoch 5, Batch 190, Loss: 0.1141\n",
      "Epoch 5, Batch 200, Loss: 0.0813\n",
      "Epoch 5, Batch 210, Loss: 0.1096\n",
      "Epoch 5, Batch 220, Loss: 0.0955\n",
      "Epoch 5, Batch 230, Loss: 0.1059\n",
      "Epoch 5, Batch 240, Loss: 0.1324\n",
      "Epoch 5, Batch 250, Loss: 0.1219\n",
      "Epoch 5, Batch 260, Loss: 0.0895\n",
      "Epoch 5, Batch 270, Loss: 0.0934\n",
      "Epoch 5, Batch 280, Loss: 0.0850\n",
      "Epoch 5, Batch 290, Loss: 0.0870\n",
      "Epoch 5, Batch 300, Loss: 0.1036\n",
      "Epoch 5, Batch 310, Loss: 0.1040\n",
      "Epoch 5, Batch 320, Loss: 0.1585\n",
      "Epoch 5, Batch 330, Loss: 0.1433\n",
      "Epoch 5, Batch 340, Loss: 0.1035\n",
      "Epoch 5, Batch 350, Loss: 0.1225\n",
      "Epoch 5, Batch 360, Loss: 0.1142\n",
      "Epoch 5, Batch 370, Loss: 0.1204\n",
      "Epoch 5, Batch 380, Loss: 0.1124\n",
      "Epoch 5, Batch 390, Loss: 0.1399\n",
      "Epoch 5, Batch 400, Loss: 0.1102\n",
      "Epoch 5, Batch 410, Loss: 0.1019\n",
      "Epoch 5, Batch 420, Loss: 0.0895\n",
      "Epoch 5, Batch 430, Loss: 0.0963\n",
      "Epoch 5, Batch 440, Loss: 0.0932\n",
      "Epoch 5, Batch 450, Loss: 0.1255\n",
      "Epoch 5, Batch 460, Loss: 0.1059\n",
      "Epoch 5, Batch 470, Loss: 0.1106\n",
      "Epoch 5, Batch 480, Loss: 0.1164\n",
      "Epoch 5, Batch 490, Loss: 0.0883\n",
      "Epoch 5, Batch 500, Loss: 0.0829\n",
      "Epoch 5, Batch 510, Loss: 0.1067\n",
      "Epoch 5, Batch 520, Loss: 0.1137\n",
      "Epoch 5, Batch 530, Loss: 0.1118\n",
      "Epoch 5, Batch 540, Loss: 0.0869\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 95.67%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "train_size_2 = int(0.7 * len(data_set_2))\n",
    "test_size_2 = len(data_set_2) - train_size_2\n",
    "train_dataset_2, test_dataset_2 = random_split(data_set_2, [train_size_2, test_size_2])\n",
    "\n",
    "# Dataloader for batch training\n",
    "train_loader_2 = DataLoader(dataset=train_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "test_loader_2 = DataLoader(dataset=test_dataset_2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion_2 = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer_2 = Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_model(model_2, train_loader_2, criterion_2, optimizer_2, epochs)\n",
    "test_model(model_2, test_loader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "508e35da-43c7-45ea-85c9-f47df434bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.67%\n",
      "Precision of the network on the test images: 95.06%\n",
      "Recall of the network on the test images: 96.25%\n",
      "Specificity of the network on the test images: 96.29%\n"
     ]
    }
   ],
   "source": [
    "# Call the modified test function\n",
    "test_model_with_metrics(model_2, test_loader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fea980be-627c-4b59-8056-e022bd8de79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 25, 87.5, 12.5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model_2, \"data/sd_2_1_frogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0312072-1055-409e-8700-3e946ba0f145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 19, 90.5, 9.5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model_2, \"data/sd_2_1_cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6efdf8c9-5559-4864-9b4b-7eded1f46e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 17, 91.5, 8.5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model_2, \"data/sd_2_1_dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d117b20-becd-4f49-ba44-d709b76f93de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 90, 55.00000000000001, 45.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model_2, \"data/sd_2_1_airplanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb1ee57c-6587-40f4-8119-cf51e3cc8caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 21, 67.1875, 32.8125)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_folder(model_2, \"data/sd_2_1_dogs_with_modifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62467a12-f581-43c0-90ce-f0c193f8a0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
