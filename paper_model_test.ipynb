{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82eb82cc-2f4a-4679-b7cd-3334b9748c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def list_files_by_type(folder_path, file_type):\n",
    "    filtered_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if fnmatch.fnmatch(file, f\"*.{file_type}\"):\n",
    "            filtered_files.append(os.path.join(folder_path, file))\n",
    "    return filtered_files\n",
    "\n",
    "def process_image(args):\n",
    "    file_path, label, transform = args  # Unpack the tuple\n",
    "    image = Image.open(file_path)\n",
    "    default_transform = transforms.ToTensor()\n",
    "    tensor = default_transform(image)\n",
    "    if transform:\n",
    "        tensor = transform(tensor)\n",
    "    return tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class CIFAKEDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def extract_index_and_category(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        pattern = r\"(\\d+)(?: \\((\\d+)\\))?\\..+\"\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            category = int(match.group(2)) if match.group(2) else 0\n",
    "            return index, category\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_folder(folder_path, label, category=None, transform=None, num_processes=1):\n",
    "        print(f\"Loading folder: {folder_path}\")\n",
    "        files = list_files_by_type(folder_path, \"jpg\")\n",
    "        if category is not None:\n",
    "            files = [file for file in files if CIFAKEDataset.extract_index_and_category(file)[1] == category]\n",
    "\n",
    "        # Use process_map from tqdm.contrib.concurrent for better tqdm updates\n",
    "        results = thread_map(process_image, [(file, label, transform) for file in files], max_workers=num_processes, chunksize=1)\n",
    "\n",
    "        x = torch.stack([result[0] for result in results])\n",
    "        y = torch.stack([result[1] for result in results])\n",
    "        return x, y\n",
    "        \n",
    "    def __init__(self, folder_path, category=None, transform=None, num_processes=1):\n",
    "        label_1_folders = [\n",
    "            os.path.join(folder_path, \"train/REAL\"),\n",
    "            os.path.join(folder_path, \"test/REAL\"),\n",
    "        ]\n",
    "        label_0_folders = [\n",
    "            os.path.join(folder_path, \"train/FAKE\"),\n",
    "            os.path.join(folder_path, \"test/FAKE\"),\n",
    "        ]\n",
    "        x1, y1 = CIFAKEDataset.load_folder(label_1_folders[0], 1, category, transform, num_processes)\n",
    "        x2, y2 = CIFAKEDataset.load_folder(label_0_folders[0], 0, category, transform, num_processes)\n",
    "        x3, y3 = CIFAKEDataset.load_folder(label_1_folders[1], 1, category, transform, num_processes)\n",
    "        x4, y4 = CIFAKEDataset.load_folder(label_0_folders[1], 0, category, transform, num_processes)\n",
    "        self.x = torch.cat((x1, x2, x3, x4))\n",
    "        self.y = torch.cat((y1, y2, y3, y4))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def data_dim(self):\n",
    "        return self.x[0].size()\n",
    "    \n",
    "    def show_example(self, idx):\n",
    "        x, y = self[idx]\n",
    "        image_array = x.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f\"Label: {y}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0639d46e-3b0b-436e-bbcd-1b109a0caabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset...\n",
      "Loading folder: data/CIFAKE\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c6f8f818f48a1aaa140cdedfaa639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6aeaff1b5c047f5accddc65d9a5b5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d774dcb57345eb9dc388fe829c7ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c70e446927486592a5d6864a7ac749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 120000\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = \"data/CIFAKE\"\n",
    "file_type = \"jpg\"\n",
    "print(\"Loading CIFAKEDataset...\")\n",
    "data_set = CIFAKEDataset(folder_path, num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(data_set))\n",
    "print(\"Data dimension:\", data_set.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "data_set.show_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617de45f-6a36-400b-978a-59438f1b482e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAKEClassifier(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network model following the architecture in the provided diagram\n",
    "class CIFAKEClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAKEClassifier, self).__init__()\n",
    "        # Assuming the input image size is 32x32x3 as per the rescale block in the diagram\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Convolutional layer with 32 outputs\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer with a 2x2 window and stride 2\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Second convolutional layer with 32 outputs\n",
    "        # Flatten layer will be applied in the forward pass\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, 1)  # Final dense layer with 1 unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU activation function after first convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU activation function after second convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the dense layer\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first dense layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply sigmoid activation function for binary classification\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CIFAKEClassifier()\n",
    "\n",
    "# Print the model structure\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139a1072-cb42-4016-9715-e7fda8ec5827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Loss: 0.6805\n",
      "Epoch 1, Batch 20, Loss: 0.6325\n",
      "Epoch 1, Batch 30, Loss: 0.6000\n",
      "Epoch 1, Batch 40, Loss: 0.5568\n",
      "Epoch 1, Batch 50, Loss: 0.5504\n",
      "Epoch 1, Batch 60, Loss: 0.4823\n",
      "Epoch 1, Batch 70, Loss: 0.4850\n",
      "Epoch 1, Batch 80, Loss: 0.4870\n",
      "Epoch 1, Batch 90, Loss: 0.4819\n",
      "Epoch 1, Batch 100, Loss: 0.4628\n",
      "Epoch 1, Batch 110, Loss: 0.4661\n",
      "Epoch 1, Batch 120, Loss: 0.4671\n",
      "Epoch 1, Batch 130, Loss: 0.4529\n",
      "Epoch 1, Batch 140, Loss: 0.4114\n",
      "Epoch 1, Batch 150, Loss: 0.4294\n",
      "Epoch 1, Batch 160, Loss: 0.4345\n",
      "Epoch 1, Batch 170, Loss: 0.4435\n",
      "Epoch 1, Batch 180, Loss: 0.3969\n",
      "Epoch 1, Batch 190, Loss: 0.4089\n",
      "Epoch 1, Batch 200, Loss: 0.3996\n",
      "Epoch 1, Batch 210, Loss: 0.3946\n",
      "Epoch 1, Batch 220, Loss: 0.3505\n",
      "Epoch 1, Batch 230, Loss: 0.3503\n",
      "Epoch 1, Batch 240, Loss: 0.4405\n",
      "Epoch 1, Batch 250, Loss: 0.4254\n",
      "Epoch 1, Batch 260, Loss: 0.4154\n",
      "Epoch 1, Batch 270, Loss: 0.4043\n",
      "Epoch 1, Batch 280, Loss: 0.3813\n",
      "Epoch 1, Batch 290, Loss: 0.3805\n",
      "Epoch 1, Batch 300, Loss: 0.3794\n",
      "Epoch 1, Batch 310, Loss: 0.3252\n",
      "Epoch 1, Batch 320, Loss: 0.3522\n",
      "Epoch 1, Batch 330, Loss: 0.3442\n",
      "Epoch 1, Batch 340, Loss: 0.3656\n",
      "Epoch 1, Batch 350, Loss: 0.3419\n",
      "Epoch 1, Batch 360, Loss: 0.3535\n",
      "Epoch 1, Batch 370, Loss: 0.3403\n",
      "Epoch 1, Batch 380, Loss: 0.3327\n",
      "Epoch 1, Batch 390, Loss: 0.3582\n",
      "Epoch 1, Batch 400, Loss: 0.3499\n",
      "Epoch 1, Batch 410, Loss: 0.3283\n",
      "Epoch 1, Batch 420, Loss: 0.3452\n",
      "Epoch 1, Batch 430, Loss: 0.3120\n",
      "Epoch 1, Batch 440, Loss: 0.3349\n",
      "Epoch 1, Batch 450, Loss: 0.3008\n",
      "Epoch 1, Batch 460, Loss: 0.2987\n",
      "Epoch 1, Batch 470, Loss: 0.3374\n",
      "Epoch 1, Batch 480, Loss: 0.3370\n",
      "Epoch 1, Batch 490, Loss: 0.3221\n",
      "Epoch 1, Batch 500, Loss: 0.3166\n",
      "Epoch 1, Batch 510, Loss: 0.3481\n",
      "Epoch 1, Batch 520, Loss: 0.3797\n",
      "Epoch 1, Batch 530, Loss: 0.2880\n",
      "Epoch 1, Batch 540, Loss: 0.3516\n",
      "Epoch 1, Batch 550, Loss: 0.2955\n",
      "Epoch 1, Batch 560, Loss: 0.3233\n",
      "Epoch 1, Batch 570, Loss: 0.3437\n",
      "Epoch 1, Batch 580, Loss: 0.3705\n",
      "Epoch 1, Batch 590, Loss: 0.3291\n",
      "Epoch 1, Batch 600, Loss: 0.2917\n",
      "Epoch 1, Batch 610, Loss: 0.3487\n",
      "Epoch 1, Batch 620, Loss: 0.3315\n",
      "Epoch 1, Batch 630, Loss: 0.3300\n",
      "Epoch 1, Batch 640, Loss: 0.3379\n",
      "Epoch 1, Batch 650, Loss: 0.2817\n",
      "Epoch 1, Batch 660, Loss: 0.2689\n",
      "Epoch 1, Batch 670, Loss: 0.3090\n",
      "Epoch 1, Batch 680, Loss: 0.2748\n",
      "Epoch 1, Batch 690, Loss: 0.2736\n",
      "Epoch 1, Batch 700, Loss: 0.2985\n",
      "Epoch 1, Batch 710, Loss: 0.2879\n",
      "Epoch 1, Batch 720, Loss: 0.3023\n",
      "Epoch 1, Batch 730, Loss: 0.2518\n",
      "Epoch 1, Batch 740, Loss: 0.2800\n",
      "Epoch 1, Batch 750, Loss: 0.2738\n",
      "Epoch 1, Batch 760, Loss: 0.2245\n",
      "Epoch 1, Batch 770, Loss: 0.3172\n",
      "Epoch 1, Batch 780, Loss: 0.3046\n",
      "Epoch 1, Batch 790, Loss: 0.2573\n",
      "Epoch 1, Batch 800, Loss: 0.2990\n",
      "Epoch 1, Batch 810, Loss: 0.2929\n",
      "Epoch 1, Batch 820, Loss: 0.2814\n",
      "Epoch 1, Batch 830, Loss: 0.2496\n",
      "Epoch 1, Batch 840, Loss: 0.2641\n",
      "Epoch 1, Batch 850, Loss: 0.2499\n",
      "Epoch 1, Batch 860, Loss: 0.2403\n",
      "Epoch 1, Batch 870, Loss: 0.3222\n",
      "Epoch 1, Batch 880, Loss: 0.3113\n",
      "Epoch 1, Batch 890, Loss: 0.2791\n",
      "Epoch 1, Batch 900, Loss: 0.2808\n",
      "Epoch 1, Batch 910, Loss: 0.2796\n",
      "Epoch 1, Batch 920, Loss: 0.2816\n",
      "Epoch 1, Batch 930, Loss: 0.2756\n",
      "Epoch 1, Batch 940, Loss: 0.2548\n",
      "Epoch 1, Batch 950, Loss: 0.2401\n",
      "Epoch 1, Batch 960, Loss: 0.2333\n",
      "Epoch 1, Batch 970, Loss: 0.2256\n",
      "Epoch 1, Batch 980, Loss: 0.2949\n",
      "Epoch 1, Batch 990, Loss: 0.2881\n",
      "Epoch 1, Batch 1000, Loss: 0.2557\n",
      "Epoch 1, Batch 1010, Loss: 0.3029\n",
      "Epoch 1, Batch 1020, Loss: 0.2437\n",
      "Epoch 1, Batch 1030, Loss: 0.2702\n",
      "Epoch 1, Batch 1040, Loss: 0.2851\n",
      "Epoch 1, Batch 1050, Loss: 0.2529\n",
      "Epoch 1, Batch 1060, Loss: 0.2742\n",
      "Epoch 1, Batch 1070, Loss: 0.3043\n",
      "Epoch 1, Batch 1080, Loss: 0.2848\n",
      "Epoch 1, Batch 1090, Loss: 0.2672\n",
      "Epoch 1, Batch 1100, Loss: 0.2780\n",
      "Epoch 1, Batch 1110, Loss: 0.3247\n",
      "Epoch 1, Batch 1120, Loss: 0.3320\n",
      "Epoch 1, Batch 1130, Loss: 0.2694\n",
      "Epoch 1, Batch 1140, Loss: 0.2253\n",
      "Epoch 1, Batch 1150, Loss: 0.2740\n",
      "Epoch 1, Batch 1160, Loss: 0.2567\n",
      "Epoch 1, Batch 1170, Loss: 0.2816\n",
      "Epoch 1, Batch 1180, Loss: 0.2625\n",
      "Epoch 1, Batch 1190, Loss: 0.2198\n",
      "Epoch 1, Batch 1200, Loss: 0.2563\n",
      "Epoch 1, Batch 1210, Loss: 0.2726\n",
      "Epoch 1, Batch 1220, Loss: 0.2004\n",
      "Epoch 1, Batch 1230, Loss: 0.2396\n",
      "Epoch 1, Batch 1240, Loss: 0.2659\n",
      "Epoch 1, Batch 1250, Loss: 0.3168\n",
      "Epoch 1, Batch 1260, Loss: 0.2356\n",
      "Epoch 1, Batch 1270, Loss: 0.2568\n",
      "Epoch 1, Batch 1280, Loss: 0.2512\n",
      "Epoch 1, Batch 1290, Loss: 0.2473\n",
      "Epoch 1, Batch 1300, Loss: 0.2122\n",
      "Epoch 1, Batch 1310, Loss: 0.2314\n",
      "Epoch 2, Batch 10, Loss: 0.2478\n",
      "Epoch 2, Batch 20, Loss: 0.3142\n",
      "Epoch 2, Batch 30, Loss: 0.3222\n",
      "Epoch 2, Batch 40, Loss: 0.2868\n",
      "Epoch 2, Batch 50, Loss: 0.2615\n",
      "Epoch 2, Batch 60, Loss: 0.2609\n",
      "Epoch 2, Batch 70, Loss: 0.2758\n",
      "Epoch 2, Batch 80, Loss: 0.2325\n",
      "Epoch 2, Batch 90, Loss: 0.2368\n",
      "Epoch 2, Batch 100, Loss: 0.2769\n",
      "Epoch 2, Batch 110, Loss: 0.2575\n",
      "Epoch 2, Batch 120, Loss: 0.2363\n",
      "Epoch 2, Batch 130, Loss: 0.2326\n",
      "Epoch 2, Batch 140, Loss: 0.2498\n",
      "Epoch 2, Batch 150, Loss: 0.2222\n",
      "Epoch 2, Batch 160, Loss: 0.2487\n",
      "Epoch 2, Batch 170, Loss: 0.2527\n",
      "Epoch 2, Batch 180, Loss: 0.2367\n",
      "Epoch 2, Batch 190, Loss: 0.2281\n",
      "Epoch 2, Batch 200, Loss: 0.2412\n",
      "Epoch 2, Batch 210, Loss: 0.2324\n",
      "Epoch 2, Batch 220, Loss: 0.2214\n",
      "Epoch 2, Batch 230, Loss: 0.2643\n",
      "Epoch 2, Batch 240, Loss: 0.1927\n",
      "Epoch 2, Batch 250, Loss: 0.1993\n",
      "Epoch 2, Batch 260, Loss: 0.2321\n",
      "Epoch 2, Batch 270, Loss: 0.2396\n",
      "Epoch 2, Batch 280, Loss: 0.2557\n",
      "Epoch 2, Batch 290, Loss: 0.2437\n",
      "Epoch 2, Batch 300, Loss: 0.2189\n",
      "Epoch 2, Batch 310, Loss: 0.2536\n",
      "Epoch 2, Batch 320, Loss: 0.2537\n",
      "Epoch 2, Batch 330, Loss: 0.2075\n",
      "Epoch 2, Batch 340, Loss: 0.2013\n",
      "Epoch 2, Batch 350, Loss: 0.2632\n",
      "Epoch 2, Batch 360, Loss: 0.2360\n",
      "Epoch 2, Batch 370, Loss: 0.2673\n",
      "Epoch 2, Batch 380, Loss: 0.2208\n",
      "Epoch 2, Batch 390, Loss: 0.2502\n",
      "Epoch 2, Batch 400, Loss: 0.1809\n",
      "Epoch 2, Batch 410, Loss: 0.2032\n",
      "Epoch 2, Batch 420, Loss: 0.2372\n",
      "Epoch 2, Batch 430, Loss: 0.2187\n",
      "Epoch 2, Batch 440, Loss: 0.2538\n",
      "Epoch 2, Batch 450, Loss: 0.2513\n",
      "Epoch 2, Batch 460, Loss: 0.2381\n",
      "Epoch 2, Batch 470, Loss: 0.2308\n",
      "Epoch 2, Batch 480, Loss: 0.1960\n",
      "Epoch 2, Batch 490, Loss: 0.2087\n",
      "Epoch 2, Batch 500, Loss: 0.2294\n",
      "Epoch 2, Batch 510, Loss: 0.1689\n",
      "Epoch 2, Batch 520, Loss: 0.1900\n",
      "Epoch 2, Batch 530, Loss: 0.2221\n",
      "Epoch 2, Batch 540, Loss: 0.2193\n",
      "Epoch 2, Batch 550, Loss: 0.2134\n",
      "Epoch 2, Batch 560, Loss: 0.2618\n",
      "Epoch 2, Batch 570, Loss: 0.2201\n",
      "Epoch 2, Batch 580, Loss: 0.2153\n",
      "Epoch 2, Batch 590, Loss: 0.2158\n",
      "Epoch 2, Batch 600, Loss: 0.2216\n",
      "Epoch 2, Batch 610, Loss: 0.2281\n",
      "Epoch 2, Batch 620, Loss: 0.2172\n",
      "Epoch 2, Batch 630, Loss: 0.2164\n",
      "Epoch 2, Batch 640, Loss: 0.2176\n",
      "Epoch 2, Batch 650, Loss: 0.2429\n",
      "Epoch 2, Batch 660, Loss: 0.2422\n",
      "Epoch 2, Batch 670, Loss: 0.2474\n",
      "Epoch 2, Batch 680, Loss: 0.2232\n",
      "Epoch 2, Batch 690, Loss: 0.2111\n",
      "Epoch 2, Batch 700, Loss: 0.2404\n",
      "Epoch 2, Batch 710, Loss: 0.1738\n",
      "Epoch 2, Batch 720, Loss: 0.2278\n",
      "Epoch 2, Batch 730, Loss: 0.2003\n",
      "Epoch 2, Batch 740, Loss: 0.2070\n",
      "Epoch 2, Batch 750, Loss: 0.2261\n",
      "Epoch 2, Batch 760, Loss: 0.2142\n",
      "Epoch 2, Batch 770, Loss: 0.2108\n",
      "Epoch 2, Batch 780, Loss: 0.2008\n",
      "Epoch 2, Batch 790, Loss: 0.2450\n",
      "Epoch 2, Batch 800, Loss: 0.2425\n",
      "Epoch 2, Batch 810, Loss: 0.2441\n",
      "Epoch 2, Batch 820, Loss: 0.2238\n",
      "Epoch 2, Batch 830, Loss: 0.1980\n",
      "Epoch 2, Batch 840, Loss: 0.2298\n",
      "Epoch 2, Batch 850, Loss: 0.1997\n",
      "Epoch 2, Batch 860, Loss: 0.2216\n",
      "Epoch 2, Batch 870, Loss: 0.1959\n",
      "Epoch 2, Batch 880, Loss: 0.2040\n",
      "Epoch 2, Batch 890, Loss: 0.2156\n",
      "Epoch 2, Batch 900, Loss: 0.1933\n",
      "Epoch 2, Batch 910, Loss: 0.2063\n",
      "Epoch 2, Batch 920, Loss: 0.2162\n",
      "Epoch 2, Batch 930, Loss: 0.1984\n",
      "Epoch 2, Batch 940, Loss: 0.1989\n",
      "Epoch 2, Batch 950, Loss: 0.2207\n",
      "Epoch 2, Batch 960, Loss: 0.2137\n",
      "Epoch 2, Batch 970, Loss: 0.2104\n",
      "Epoch 2, Batch 980, Loss: 0.2303\n",
      "Epoch 2, Batch 990, Loss: 0.2089\n",
      "Epoch 2, Batch 1000, Loss: 0.2144\n",
      "Epoch 2, Batch 1010, Loss: 0.2110\n",
      "Epoch 2, Batch 1020, Loss: 0.2021\n",
      "Epoch 2, Batch 1030, Loss: 0.2181\n",
      "Epoch 2, Batch 1040, Loss: 0.2112\n",
      "Epoch 2, Batch 1050, Loss: 0.1968\n",
      "Epoch 2, Batch 1060, Loss: 0.1921\n",
      "Epoch 2, Batch 1070, Loss: 0.1954\n",
      "Epoch 2, Batch 1080, Loss: 0.1912\n",
      "Epoch 2, Batch 1090, Loss: 0.1814\n",
      "Epoch 2, Batch 1100, Loss: 0.2613\n",
      "Epoch 2, Batch 1110, Loss: 0.2198\n",
      "Epoch 2, Batch 1120, Loss: 0.2225\n",
      "Epoch 2, Batch 1130, Loss: 0.1844\n",
      "Epoch 2, Batch 1140, Loss: 0.2232\n",
      "Epoch 2, Batch 1150, Loss: 0.2136\n",
      "Epoch 2, Batch 1160, Loss: 0.1933\n",
      "Epoch 2, Batch 1170, Loss: 0.1909\n",
      "Epoch 2, Batch 1180, Loss: 0.1654\n",
      "Epoch 2, Batch 1190, Loss: 0.1934\n",
      "Epoch 2, Batch 1200, Loss: 0.2107\n",
      "Epoch 2, Batch 1210, Loss: 0.1892\n",
      "Epoch 2, Batch 1220, Loss: 0.1452\n",
      "Epoch 2, Batch 1230, Loss: 0.1891\n",
      "Epoch 2, Batch 1240, Loss: 0.1959\n",
      "Epoch 2, Batch 1250, Loss: 0.2375\n",
      "Epoch 2, Batch 1260, Loss: 0.1535\n",
      "Epoch 2, Batch 1270, Loss: 0.1881\n",
      "Epoch 2, Batch 1280, Loss: 0.1760\n",
      "Epoch 2, Batch 1290, Loss: 0.1584\n",
      "Epoch 2, Batch 1300, Loss: 0.1811\n",
      "Epoch 2, Batch 1310, Loss: 0.2052\n",
      "Epoch 3, Batch 10, Loss: 0.1673\n",
      "Epoch 3, Batch 20, Loss: 0.2020\n",
      "Epoch 3, Batch 30, Loss: 0.1872\n",
      "Epoch 3, Batch 40, Loss: 0.1828\n",
      "Epoch 3, Batch 50, Loss: 0.1826\n",
      "Epoch 3, Batch 60, Loss: 0.2188\n",
      "Epoch 3, Batch 70, Loss: 0.2151\n",
      "Epoch 3, Batch 80, Loss: 0.1957\n",
      "Epoch 3, Batch 90, Loss: 0.2108\n",
      "Epoch 3, Batch 100, Loss: 0.1762\n",
      "Epoch 3, Batch 110, Loss: 0.1969\n",
      "Epoch 3, Batch 120, Loss: 0.1556\n",
      "Epoch 3, Batch 130, Loss: 0.1952\n",
      "Epoch 3, Batch 140, Loss: 0.1784\n",
      "Epoch 3, Batch 150, Loss: 0.2081\n",
      "Epoch 3, Batch 160, Loss: 0.2290\n",
      "Epoch 3, Batch 170, Loss: 0.2384\n",
      "Epoch 3, Batch 180, Loss: 0.1706\n",
      "Epoch 3, Batch 190, Loss: 0.2060\n",
      "Epoch 3, Batch 200, Loss: 0.1711\n",
      "Epoch 3, Batch 210, Loss: 0.2135\n",
      "Epoch 3, Batch 220, Loss: 0.1823\n",
      "Epoch 3, Batch 230, Loss: 0.1943\n",
      "Epoch 3, Batch 240, Loss: 0.1729\n",
      "Epoch 3, Batch 250, Loss: 0.2224\n",
      "Epoch 3, Batch 260, Loss: 0.2213\n",
      "Epoch 3, Batch 270, Loss: 0.1841\n",
      "Epoch 3, Batch 280, Loss: 0.1732\n",
      "Epoch 3, Batch 290, Loss: 0.2088\n",
      "Epoch 3, Batch 300, Loss: 0.1852\n",
      "Epoch 3, Batch 310, Loss: 0.1764\n",
      "Epoch 3, Batch 320, Loss: 0.1755\n",
      "Epoch 3, Batch 330, Loss: 0.1730\n",
      "Epoch 3, Batch 340, Loss: 0.1539\n",
      "Epoch 3, Batch 350, Loss: 0.1853\n",
      "Epoch 3, Batch 360, Loss: 0.1619\n",
      "Epoch 3, Batch 370, Loss: 0.2225\n",
      "Epoch 3, Batch 380, Loss: 0.1936\n",
      "Epoch 3, Batch 390, Loss: 0.1780\n",
      "Epoch 3, Batch 400, Loss: 0.2341\n",
      "Epoch 3, Batch 410, Loss: 0.2188\n",
      "Epoch 3, Batch 420, Loss: 0.1904\n",
      "Epoch 3, Batch 430, Loss: 0.2114\n",
      "Epoch 3, Batch 440, Loss: 0.1871\n",
      "Epoch 3, Batch 450, Loss: 0.1812\n",
      "Epoch 3, Batch 460, Loss: 0.1715\n",
      "Epoch 3, Batch 470, Loss: 0.1932\n",
      "Epoch 3, Batch 480, Loss: 0.1583\n",
      "Epoch 3, Batch 490, Loss: 0.1563\n",
      "Epoch 3, Batch 500, Loss: 0.1694\n",
      "Epoch 3, Batch 510, Loss: 0.2317\n",
      "Epoch 3, Batch 520, Loss: 0.2495\n",
      "Epoch 3, Batch 530, Loss: 0.1576\n",
      "Epoch 3, Batch 540, Loss: 0.1710\n",
      "Epoch 3, Batch 550, Loss: 0.1834\n",
      "Epoch 3, Batch 560, Loss: 0.2101\n",
      "Epoch 3, Batch 570, Loss: 0.1762\n",
      "Epoch 3, Batch 580, Loss: 0.1678\n",
      "Epoch 3, Batch 590, Loss: 0.2210\n",
      "Epoch 3, Batch 600, Loss: 0.1882\n",
      "Epoch 3, Batch 610, Loss: 0.2060\n",
      "Epoch 3, Batch 620, Loss: 0.1720\n",
      "Epoch 3, Batch 630, Loss: 0.2042\n",
      "Epoch 3, Batch 640, Loss: 0.1580\n",
      "Epoch 3, Batch 650, Loss: 0.1782\n",
      "Epoch 3, Batch 660, Loss: 0.1744\n",
      "Epoch 3, Batch 670, Loss: 0.1581\n",
      "Epoch 3, Batch 680, Loss: 0.1604\n",
      "Epoch 3, Batch 690, Loss: 0.1431\n",
      "Epoch 3, Batch 700, Loss: 0.1484\n",
      "Epoch 3, Batch 710, Loss: 0.1948\n",
      "Epoch 3, Batch 720, Loss: 0.1956\n",
      "Epoch 3, Batch 730, Loss: 0.2030\n",
      "Epoch 3, Batch 740, Loss: 0.1956\n",
      "Epoch 3, Batch 750, Loss: 0.1759\n",
      "Epoch 3, Batch 760, Loss: 0.1746\n",
      "Epoch 3, Batch 770, Loss: 0.1722\n",
      "Epoch 3, Batch 780, Loss: 0.1526\n",
      "Epoch 3, Batch 790, Loss: 0.1696\n",
      "Epoch 3, Batch 800, Loss: 0.1854\n",
      "Epoch 3, Batch 810, Loss: 0.1988\n",
      "Epoch 3, Batch 820, Loss: 0.1891\n",
      "Epoch 3, Batch 830, Loss: 0.2190\n",
      "Epoch 3, Batch 840, Loss: 0.2102\n",
      "Epoch 3, Batch 850, Loss: 0.1772\n",
      "Epoch 3, Batch 860, Loss: 0.2182\n",
      "Epoch 3, Batch 870, Loss: 0.1777\n",
      "Epoch 3, Batch 880, Loss: 0.2021\n",
      "Epoch 3, Batch 890, Loss: 0.1638\n",
      "Epoch 3, Batch 900, Loss: 0.1527\n",
      "Epoch 3, Batch 910, Loss: 0.1848\n",
      "Epoch 3, Batch 920, Loss: 0.1930\n",
      "Epoch 3, Batch 930, Loss: 0.1741\n",
      "Epoch 3, Batch 940, Loss: 0.2029\n",
      "Epoch 3, Batch 950, Loss: 0.1863\n",
      "Epoch 3, Batch 960, Loss: 0.2032\n",
      "Epoch 3, Batch 970, Loss: 0.1847\n",
      "Epoch 3, Batch 980, Loss: 0.2104\n",
      "Epoch 3, Batch 990, Loss: 0.1637\n",
      "Epoch 3, Batch 1000, Loss: 0.1775\n",
      "Epoch 3, Batch 1010, Loss: 0.2054\n",
      "Epoch 3, Batch 1020, Loss: 0.1516\n",
      "Epoch 3, Batch 1030, Loss: 0.1860\n",
      "Epoch 3, Batch 1040, Loss: 0.1845\n",
      "Epoch 3, Batch 1050, Loss: 0.1638\n",
      "Epoch 3, Batch 1060, Loss: 0.1818\n",
      "Epoch 3, Batch 1070, Loss: 0.1874\n",
      "Epoch 3, Batch 1080, Loss: 0.1835\n",
      "Epoch 3, Batch 1090, Loss: 0.1738\n",
      "Epoch 3, Batch 1100, Loss: 0.1536\n",
      "Epoch 3, Batch 1110, Loss: 0.1562\n",
      "Epoch 3, Batch 1120, Loss: 0.1879\n",
      "Epoch 3, Batch 1130, Loss: 0.1587\n",
      "Epoch 3, Batch 1140, Loss: 0.1796\n",
      "Epoch 3, Batch 1150, Loss: 0.1991\n",
      "Epoch 3, Batch 1160, Loss: 0.2002\n",
      "Epoch 3, Batch 1170, Loss: 0.1857\n",
      "Epoch 3, Batch 1180, Loss: 0.1792\n",
      "Epoch 3, Batch 1190, Loss: 0.2029\n",
      "Epoch 3, Batch 1200, Loss: 0.1358\n",
      "Epoch 3, Batch 1210, Loss: 0.1650\n",
      "Epoch 3, Batch 1220, Loss: 0.1410\n",
      "Epoch 3, Batch 1230, Loss: 0.2061\n",
      "Epoch 3, Batch 1240, Loss: 0.1710\n",
      "Epoch 3, Batch 1250, Loss: 0.1366\n",
      "Epoch 3, Batch 1260, Loss: 0.1563\n",
      "Epoch 3, Batch 1270, Loss: 0.1811\n",
      "Epoch 3, Batch 1280, Loss: 0.1536\n",
      "Epoch 3, Batch 1290, Loss: 0.1710\n",
      "Epoch 3, Batch 1300, Loss: 0.1948\n",
      "Epoch 3, Batch 1310, Loss: 0.1749\n",
      "Epoch 4, Batch 10, Loss: 0.1766\n",
      "Epoch 4, Batch 20, Loss: 0.1423\n",
      "Epoch 4, Batch 30, Loss: 0.1662\n",
      "Epoch 4, Batch 40, Loss: 0.1247\n",
      "Epoch 4, Batch 50, Loss: 0.1171\n",
      "Epoch 4, Batch 60, Loss: 0.1587\n",
      "Epoch 4, Batch 70, Loss: 0.1589\n",
      "Epoch 4, Batch 80, Loss: 0.1971\n",
      "Epoch 4, Batch 90, Loss: 0.1760\n",
      "Epoch 4, Batch 100, Loss: 0.1761\n",
      "Epoch 4, Batch 110, Loss: 0.1625\n",
      "Epoch 4, Batch 120, Loss: 0.1541\n",
      "Epoch 4, Batch 130, Loss: 0.1524\n",
      "Epoch 4, Batch 140, Loss: 0.1851\n",
      "Epoch 4, Batch 150, Loss: 0.1367\n",
      "Epoch 4, Batch 160, Loss: 0.1472\n",
      "Epoch 4, Batch 170, Loss: 0.1650\n",
      "Epoch 4, Batch 180, Loss: 0.1828\n",
      "Epoch 4, Batch 190, Loss: 0.1356\n",
      "Epoch 4, Batch 200, Loss: 0.1877\n",
      "Epoch 4, Batch 210, Loss: 0.1577\n",
      "Epoch 4, Batch 220, Loss: 0.1359\n",
      "Epoch 4, Batch 230, Loss: 0.1599\n",
      "Epoch 4, Batch 240, Loss: 0.1546\n",
      "Epoch 4, Batch 250, Loss: 0.1934\n",
      "Epoch 4, Batch 260, Loss: 0.1591\n",
      "Epoch 4, Batch 270, Loss: 0.1559\n",
      "Epoch 4, Batch 280, Loss: 0.1409\n",
      "Epoch 4, Batch 290, Loss: 0.1596\n",
      "Epoch 4, Batch 300, Loss: 0.1754\n",
      "Epoch 4, Batch 310, Loss: 0.1445\n",
      "Epoch 4, Batch 320, Loss: 0.1739\n",
      "Epoch 4, Batch 330, Loss: 0.1569\n",
      "Epoch 4, Batch 340, Loss: 0.1564\n",
      "Epoch 4, Batch 350, Loss: 0.1582\n",
      "Epoch 4, Batch 360, Loss: 0.1979\n",
      "Epoch 4, Batch 370, Loss: 0.1652\n",
      "Epoch 4, Batch 380, Loss: 0.1575\n",
      "Epoch 4, Batch 390, Loss: 0.1660\n",
      "Epoch 4, Batch 400, Loss: 0.1560\n",
      "Epoch 4, Batch 410, Loss: 0.1718\n",
      "Epoch 4, Batch 420, Loss: 0.1601\n",
      "Epoch 4, Batch 430, Loss: 0.1800\n",
      "Epoch 4, Batch 440, Loss: 0.1850\n",
      "Epoch 4, Batch 450, Loss: 0.1420\n",
      "Epoch 4, Batch 460, Loss: 0.1304\n",
      "Epoch 4, Batch 470, Loss: 0.1542\n",
      "Epoch 4, Batch 480, Loss: 0.1680\n",
      "Epoch 4, Batch 490, Loss: 0.1613\n",
      "Epoch 4, Batch 500, Loss: 0.1970\n",
      "Epoch 4, Batch 510, Loss: 0.1536\n",
      "Epoch 4, Batch 520, Loss: 0.1442\n",
      "Epoch 4, Batch 530, Loss: 0.1568\n",
      "Epoch 4, Batch 540, Loss: 0.1323\n",
      "Epoch 4, Batch 550, Loss: 0.1676\n",
      "Epoch 4, Batch 560, Loss: 0.1912\n",
      "Epoch 4, Batch 570, Loss: 0.1443\n",
      "Epoch 4, Batch 580, Loss: 0.1643\n",
      "Epoch 4, Batch 590, Loss: 0.1664\n",
      "Epoch 4, Batch 600, Loss: 0.1635\n",
      "Epoch 4, Batch 610, Loss: 0.1522\n",
      "Epoch 4, Batch 620, Loss: 0.1491\n",
      "Epoch 4, Batch 630, Loss: 0.1821\n",
      "Epoch 4, Batch 640, Loss: 0.1692\n",
      "Epoch 4, Batch 650, Loss: 0.1760\n",
      "Epoch 4, Batch 660, Loss: 0.1635\n",
      "Epoch 4, Batch 670, Loss: 0.1662\n",
      "Epoch 4, Batch 680, Loss: 0.1542\n",
      "Epoch 4, Batch 690, Loss: 0.1524\n",
      "Epoch 4, Batch 700, Loss: 0.1675\n",
      "Epoch 4, Batch 710, Loss: 0.1386\n",
      "Epoch 4, Batch 720, Loss: 0.2025\n",
      "Epoch 4, Batch 730, Loss: 0.1874\n",
      "Epoch 4, Batch 740, Loss: 0.1413\n",
      "Epoch 4, Batch 750, Loss: 0.1967\n",
      "Epoch 4, Batch 760, Loss: 0.1593\n",
      "Epoch 4, Batch 770, Loss: 0.1675\n",
      "Epoch 4, Batch 780, Loss: 0.1819\n",
      "Epoch 4, Batch 790, Loss: 0.1878\n",
      "Epoch 4, Batch 800, Loss: 0.1528\n",
      "Epoch 4, Batch 810, Loss: 0.1681\n",
      "Epoch 4, Batch 820, Loss: 0.1945\n",
      "Epoch 4, Batch 830, Loss: 0.1732\n",
      "Epoch 4, Batch 840, Loss: 0.1842\n",
      "Epoch 4, Batch 850, Loss: 0.1557\n",
      "Epoch 4, Batch 860, Loss: 0.1603\n",
      "Epoch 4, Batch 870, Loss: 0.1427\n",
      "Epoch 4, Batch 880, Loss: 0.1586\n",
      "Epoch 4, Batch 890, Loss: 0.1451\n",
      "Epoch 4, Batch 900, Loss: 0.1813\n",
      "Epoch 4, Batch 910, Loss: 0.1660\n",
      "Epoch 4, Batch 920, Loss: 0.1617\n",
      "Epoch 4, Batch 930, Loss: 0.1442\n",
      "Epoch 4, Batch 940, Loss: 0.1747\n",
      "Epoch 4, Batch 950, Loss: 0.1533\n",
      "Epoch 4, Batch 960, Loss: 0.1550\n",
      "Epoch 4, Batch 970, Loss: 0.2013\n",
      "Epoch 4, Batch 980, Loss: 0.1671\n",
      "Epoch 4, Batch 990, Loss: 0.1446\n",
      "Epoch 4, Batch 1000, Loss: 0.1819\n",
      "Epoch 4, Batch 1010, Loss: 0.2058\n",
      "Epoch 4, Batch 1020, Loss: 0.1749\n",
      "Epoch 4, Batch 1030, Loss: 0.1770\n",
      "Epoch 4, Batch 1040, Loss: 0.1594\n",
      "Epoch 4, Batch 1050, Loss: 0.1797\n",
      "Epoch 4, Batch 1060, Loss: 0.1567\n",
      "Epoch 4, Batch 1070, Loss: 0.1462\n",
      "Epoch 4, Batch 1080, Loss: 0.1512\n",
      "Epoch 4, Batch 1090, Loss: 0.1551\n",
      "Epoch 4, Batch 1100, Loss: 0.1746\n",
      "Epoch 4, Batch 1110, Loss: 0.1367\n",
      "Epoch 4, Batch 1120, Loss: 0.2201\n",
      "Epoch 4, Batch 1130, Loss: 0.1821\n",
      "Epoch 4, Batch 1140, Loss: 0.1578\n",
      "Epoch 4, Batch 1150, Loss: 0.1328\n",
      "Epoch 4, Batch 1160, Loss: 0.1475\n",
      "Epoch 4, Batch 1170, Loss: 0.1873\n",
      "Epoch 4, Batch 1180, Loss: 0.1660\n",
      "Epoch 4, Batch 1190, Loss: 0.1858\n",
      "Epoch 4, Batch 1200, Loss: 0.1549\n",
      "Epoch 4, Batch 1210, Loss: 0.1470\n",
      "Epoch 4, Batch 1220, Loss: 0.1577\n",
      "Epoch 4, Batch 1230, Loss: 0.1543\n",
      "Epoch 4, Batch 1240, Loss: 0.1544\n",
      "Epoch 4, Batch 1250, Loss: 0.1554\n",
      "Epoch 4, Batch 1260, Loss: 0.1503\n",
      "Epoch 4, Batch 1270, Loss: 0.1546\n",
      "Epoch 4, Batch 1280, Loss: 0.1539\n",
      "Epoch 4, Batch 1290, Loss: 0.1849\n",
      "Epoch 4, Batch 1300, Loss: 0.1490\n",
      "Epoch 4, Batch 1310, Loss: 0.1993\n",
      "Epoch 5, Batch 10, Loss: 0.1407\n",
      "Epoch 5, Batch 20, Loss: 0.1291\n",
      "Epoch 5, Batch 30, Loss: 0.1430\n",
      "Epoch 5, Batch 40, Loss: 0.1541\n",
      "Epoch 5, Batch 50, Loss: 0.1874\n",
      "Epoch 5, Batch 60, Loss: 0.1547\n",
      "Epoch 5, Batch 70, Loss: 0.1384\n",
      "Epoch 5, Batch 80, Loss: 0.1600\n",
      "Epoch 5, Batch 90, Loss: 0.1645\n",
      "Epoch 5, Batch 100, Loss: 0.1565\n",
      "Epoch 5, Batch 110, Loss: 0.1550\n",
      "Epoch 5, Batch 120, Loss: 0.1637\n",
      "Epoch 5, Batch 130, Loss: 0.1842\n",
      "Epoch 5, Batch 140, Loss: 0.1242\n",
      "Epoch 5, Batch 150, Loss: 0.1696\n",
      "Epoch 5, Batch 160, Loss: 0.1059\n",
      "Epoch 5, Batch 170, Loss: 0.1649\n",
      "Epoch 5, Batch 180, Loss: 0.1463\n",
      "Epoch 5, Batch 190, Loss: 0.1615\n",
      "Epoch 5, Batch 200, Loss: 0.1585\n",
      "Epoch 5, Batch 210, Loss: 0.1641\n",
      "Epoch 5, Batch 220, Loss: 0.1494\n",
      "Epoch 5, Batch 230, Loss: 0.1743\n",
      "Epoch 5, Batch 240, Loss: 0.1296\n",
      "Epoch 5, Batch 250, Loss: 0.1519\n",
      "Epoch 5, Batch 260, Loss: 0.1359\n",
      "Epoch 5, Batch 270, Loss: 0.1495\n",
      "Epoch 5, Batch 280, Loss: 0.1420\n",
      "Epoch 5, Batch 290, Loss: 0.2324\n",
      "Epoch 5, Batch 300, Loss: 0.1726\n",
      "Epoch 5, Batch 310, Loss: 0.1618\n",
      "Epoch 5, Batch 320, Loss: 0.1663\n",
      "Epoch 5, Batch 330, Loss: 0.1519\n",
      "Epoch 5, Batch 340, Loss: 0.1497\n",
      "Epoch 5, Batch 350, Loss: 0.1552\n",
      "Epoch 5, Batch 360, Loss: 0.1397\n",
      "Epoch 5, Batch 370, Loss: 0.1741\n",
      "Epoch 5, Batch 380, Loss: 0.1249\n",
      "Epoch 5, Batch 390, Loss: 0.1472\n",
      "Epoch 5, Batch 400, Loss: 0.1287\n",
      "Epoch 5, Batch 410, Loss: 0.1464\n",
      "Epoch 5, Batch 420, Loss: 0.1796\n",
      "Epoch 5, Batch 430, Loss: 0.1466\n",
      "Epoch 5, Batch 440, Loss: 0.1542\n",
      "Epoch 5, Batch 450, Loss: 0.1478\n",
      "Epoch 5, Batch 460, Loss: 0.1517\n",
      "Epoch 5, Batch 470, Loss: 0.1881\n",
      "Epoch 5, Batch 480, Loss: 0.1358\n",
      "Epoch 5, Batch 490, Loss: 0.1607\n",
      "Epoch 5, Batch 500, Loss: 0.1626\n",
      "Epoch 5, Batch 510, Loss: 0.1776\n",
      "Epoch 5, Batch 520, Loss: 0.1503\n",
      "Epoch 5, Batch 530, Loss: 0.1258\n",
      "Epoch 5, Batch 540, Loss: 0.1294\n",
      "Epoch 5, Batch 550, Loss: 0.1348\n",
      "Epoch 5, Batch 560, Loss: 0.1658\n",
      "Epoch 5, Batch 570, Loss: 0.1674\n",
      "Epoch 5, Batch 580, Loss: 0.1450\n",
      "Epoch 5, Batch 590, Loss: 0.1661\n",
      "Epoch 5, Batch 600, Loss: 0.1586\n",
      "Epoch 5, Batch 610, Loss: 0.1285\n",
      "Epoch 5, Batch 620, Loss: 0.1305\n",
      "Epoch 5, Batch 630, Loss: 0.1805\n",
      "Epoch 5, Batch 640, Loss: 0.1436\n",
      "Epoch 5, Batch 650, Loss: 0.1522\n",
      "Epoch 5, Batch 660, Loss: 0.1659\n",
      "Epoch 5, Batch 670, Loss: 0.1329\n",
      "Epoch 5, Batch 680, Loss: 0.1438\n",
      "Epoch 5, Batch 690, Loss: 0.1554\n",
      "Epoch 5, Batch 700, Loss: 0.1452\n",
      "Epoch 5, Batch 710, Loss: 0.1404\n",
      "Epoch 5, Batch 720, Loss: 0.1588\n",
      "Epoch 5, Batch 730, Loss: 0.1842\n",
      "Epoch 5, Batch 740, Loss: 0.1650\n",
      "Epoch 5, Batch 750, Loss: 0.1553\n",
      "Epoch 5, Batch 760, Loss: 0.1583\n",
      "Epoch 5, Batch 770, Loss: 0.1264\n",
      "Epoch 5, Batch 780, Loss: 0.1688\n",
      "Epoch 5, Batch 790, Loss: 0.1104\n",
      "Epoch 5, Batch 800, Loss: 0.1386\n",
      "Epoch 5, Batch 810, Loss: 0.1375\n",
      "Epoch 5, Batch 820, Loss: 0.1325\n",
      "Epoch 5, Batch 830, Loss: 0.1449\n",
      "Epoch 5, Batch 840, Loss: 0.1470\n",
      "Epoch 5, Batch 850, Loss: 0.1381\n",
      "Epoch 5, Batch 860, Loss: 0.1435\n",
      "Epoch 5, Batch 870, Loss: 0.1465\n",
      "Epoch 5, Batch 880, Loss: 0.1474\n",
      "Epoch 5, Batch 890, Loss: 0.1243\n",
      "Epoch 5, Batch 900, Loss: 0.1509\n",
      "Epoch 5, Batch 910, Loss: 0.1514\n",
      "Epoch 5, Batch 920, Loss: 0.1371\n",
      "Epoch 5, Batch 930, Loss: 0.1587\n",
      "Epoch 5, Batch 940, Loss: 0.1729\n",
      "Epoch 5, Batch 950, Loss: 0.1226\n",
      "Epoch 5, Batch 960, Loss: 0.1704\n",
      "Epoch 5, Batch 970, Loss: 0.1455\n",
      "Epoch 5, Batch 980, Loss: 0.1536\n",
      "Epoch 5, Batch 990, Loss: 0.1703\n",
      "Epoch 5, Batch 1000, Loss: 0.1357\n",
      "Epoch 5, Batch 1010, Loss: 0.1272\n",
      "Epoch 5, Batch 1020, Loss: 0.1332\n",
      "Epoch 5, Batch 1030, Loss: 0.1792\n",
      "Epoch 5, Batch 1040, Loss: 0.1420\n",
      "Epoch 5, Batch 1050, Loss: 0.1585\n",
      "Epoch 5, Batch 1060, Loss: 0.1636\n",
      "Epoch 5, Batch 1070, Loss: 0.1477\n",
      "Epoch 5, Batch 1080, Loss: 0.1480\n",
      "Epoch 5, Batch 1090, Loss: 0.1159\n",
      "Epoch 5, Batch 1100, Loss: 0.1356\n",
      "Epoch 5, Batch 1110, Loss: 0.1673\n",
      "Epoch 5, Batch 1120, Loss: 0.1415\n",
      "Epoch 5, Batch 1130, Loss: 0.1288\n",
      "Epoch 5, Batch 1140, Loss: 0.1300\n",
      "Epoch 5, Batch 1150, Loss: 0.1394\n",
      "Epoch 5, Batch 1160, Loss: 0.1519\n",
      "Epoch 5, Batch 1170, Loss: 0.1741\n",
      "Epoch 5, Batch 1180, Loss: 0.1283\n",
      "Epoch 5, Batch 1190, Loss: 0.1500\n",
      "Epoch 5, Batch 1200, Loss: 0.1272\n",
      "Epoch 5, Batch 1210, Loss: 0.1470\n",
      "Epoch 5, Batch 1220, Loss: 0.1513\n",
      "Epoch 5, Batch 1230, Loss: 0.1262\n",
      "Epoch 5, Batch 1240, Loss: 0.1466\n",
      "Epoch 5, Batch 1250, Loss: 0.1458\n",
      "Epoch 5, Batch 1260, Loss: 0.1533\n",
      "Epoch 5, Batch 1270, Loss: 0.1393\n",
      "Epoch 5, Batch 1280, Loss: 0.1694\n",
      "Epoch 5, Batch 1290, Loss: 0.1086\n",
      "Epoch 5, Batch 1300, Loss: 0.1341\n",
      "Epoch 5, Batch 1310, Loss: 0.1339\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 94.13%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "# Create the training and testing splits\n",
    "train_size = int(0.7 * len(data_set))\n",
    "test_size = len(data_set) - train_size\n",
    "train_dataset, test_dataset = random_split(data_set, [train_size, test_size])\n",
    "\n",
    "# Dataloader for batch training\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()  # BCELoss expects labels to be in float format\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()  # Remove unnecessary dimensions\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:  # Print every 10 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_model(model, train_loader, criterion, optimizer, epochs)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99d4e9-acfe-44d3-88b2-90a5b30f75f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
