{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c181e285-4131-4c59-8094-6c510c17388e",
   "metadata": {},
   "source": [
    "## Step 1: Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e59f121-586b-48a2-b502-d25d44ef92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def list_files_by_type(folder_path, file_type):\n",
    "    filtered_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if fnmatch.fnmatch(file, f\"*.{file_type}\"):\n",
    "            filtered_files.append(os.path.join(folder_path, file))\n",
    "    return filtered_files\n",
    "\n",
    "def process_image(args):\n",
    "    file_path, label, transform = args  # Unpack the tuple\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    default_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    tensor = default_transform(image)\n",
    "    if transform:\n",
    "        tensor = transform(tensor)\n",
    "    return tensor, torch.tensor(label, dtype=torch.long), file_path\n",
    "\n",
    "class CIFAKEDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def extract_index_and_category(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        pattern = r\"(\\d+)(?: \\((\\d+)\\))?\\..+\"\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            category = int(match.group(2)) if match.group(2) else 0\n",
    "            return index, category\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_folder(folder_path, label, category=None, transform=None, num_processes=1):\n",
    "        print(f\"Loading folder: {folder_path}\")\n",
    "        files = list_files_by_type(folder_path, \"jpg\")\n",
    "        if category is not None:\n",
    "            files = [file for file in files if CIFAKEDataset.extract_index_and_category(file)[1] == category]\n",
    "\n",
    "        # Use process_map from tqdm.contrib.concurrent for better tqdm updates\n",
    "        results = thread_map(process_image, [(file, label, transform) for file in files], max_workers=num_processes, chunksize=1)\n",
    "\n",
    "        x = torch.stack([result[0] for result in results])\n",
    "        y = torch.stack([result[1] for result in results])\n",
    "        file_paths = [result[2] for result in results]\n",
    "        return x, y, file_paths\n",
    "        \n",
    "    def __init__(self, folder_path, category=None, transform=None, num_processes=1, use_cifake_fold_structure=True):\n",
    "        self.latest_file_path = None\n",
    "        \n",
    "        if use_cifake_fold_structure:\n",
    "            label_1_folders = [\n",
    "                os.path.join(folder_path, \"train/REAL\"),\n",
    "                os.path.join(folder_path, \"test/REAL\"),\n",
    "            ]\n",
    "            label_0_folders = [\n",
    "                os.path.join(folder_path, \"train/FAKE\"),\n",
    "                os.path.join(folder_path, \"test/FAKE\"),\n",
    "            ]\n",
    "            x1, y1, fp_1 = CIFAKEDataset.load_folder(label_1_folders[0], 1, category, transform, num_processes)\n",
    "            x2, y2, fp_2 = CIFAKEDataset.load_folder(label_0_folders[0], 0, category, transform, num_processes)\n",
    "            x3, y3, fp_3 = CIFAKEDataset.load_folder(label_1_folders[1], 1, category, transform, num_processes)\n",
    "            x4, y4, fp_4 = CIFAKEDataset.load_folder(label_0_folders[1], 0, category, transform, num_processes)\n",
    "            self.x = torch.cat((x1, x2, x3, x4))\n",
    "            self.y = torch.cat((y1, y2, y3, y4))\n",
    "            self.file_paths = fp_1 + fp_2 + fp_3 + fp_4\n",
    "        else:\n",
    "            self.x, self.y, self.file_paths = CIFAKEDataset.load_folder(folder_path, 0, None, transform, num_processes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.latest_file_path = self.file_paths[idx]\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def data_dim(self):\n",
    "        return self.x[0].size()\n",
    "    \n",
    "    def show_example(self, idx):\n",
    "        x, y = self[idx]\n",
    "        image_array = x.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f\"Label: {y}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def latest_file_path(self):\n",
    "        return self.latest_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d5a3a-e904-4b5b-ba0a-aa591661adda",
   "metadata": {},
   "source": [
    "## Step 2: Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240f413-e894-4c54-bb83-edf9d3c385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network model following the architecture in the provided diagram\n",
    "class CIFAKEClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAKEClassifier, self).__init__()\n",
    "        # Assuming the input image size is 32x32x3 as per the rescale block in the diagram\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Convolutional layer with 32 outputs\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer with a 2x2 window and stride 2\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Second convolutional layer with 32 outputs\n",
    "        # Flatten layer will be applied in the forward pass\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, 1)  # Final dense layer with 1 unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU activation function after first convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU activation function after second convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the dense layer\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first dense layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply sigmoid activation function for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fd1d9-1842-43cc-99e4-3408d3c6437b",
   "metadata": {},
   "source": [
    "## Step 3: Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc8a162-ba0e-4a3d-9bdf-685985441d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "def make_dataloader(dataset, training_data_ratio=0.7):\n",
    "    # Create the training and testing splits\n",
    "    train_size = int(training_data_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Dataloader for batch training\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, dataloader, epochs):\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()  # BCELoss expects labels to be in float format\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()  # Remove unnecessary dimensions\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# Test the model and calculate precision, recall, and specificity\n",
    "def test_model_with_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = 100 * precision_score(all_labels, all_predictions)\n",
    "    recall = 100 * recall_score(all_labels, all_predictions)\n",
    "\n",
    "    # Calculate specificity\n",
    "    TN = ((1 - np.array(all_predictions)) * (1 - np.array(all_labels))).sum()\n",
    "    FN = ((1 - np.array(all_predictions)) * np.array(all_labels)).sum()\n",
    "    specificity = 100 * TN / (TN + FN)\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    print(f'Precision of the network on the test images: {precision:.2f}%')\n",
    "    print(f'Recall of the network on the test images: {recall:.2f}%')\n",
    "    print(f'Specificity of the network on the test images: {specificity:.2f}%')\n",
    "\n",
    "def evaluate_folder(model, folder_path):\n",
    "    \"\"\"Evaluate a single folder of images and return the counts and rates.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    total_images = 0\n",
    "\n",
    "    image_list = os.listdir(folder_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    for image_name in tqdm(image_list, desc=\"Evaluating images\"):\n",
    "        try:\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to load one image {image_path}, Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(output)  # Round to get binary predictions\n",
    "            label = predicted.item()\n",
    "\n",
    "        if label == 0:\n",
    "            count_0 += 1\n",
    "        else:\n",
    "            count_1 += 1\n",
    "\n",
    "        total_images += 1\n",
    "\n",
    "    rate_0 = (count_0 / total_images) * 100 if total_images > 0 else 0\n",
    "    rate_1 = (count_1 / total_images) * 100 if total_images > 0 else 0\n",
    "\n",
    "    return count_0, count_1, rate_0, rate_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e1329-3d21-4c27-a3db-89dc5df4b33e",
   "metadata": {},
   "source": [
    "## Step 4: Load Original CIFAKE Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba49d85-a39b-438a-a695-f1ec5e4de58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset...\n",
      "Loading folder: data/CIFAKE\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ba8079f2dd4faf8c0a10fb9f470a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f268381ceae4938862e24fdc364d95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a25bf63566844868b414cbb12d2de5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37924b72bbf444139538afd78c2b51ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 120000\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 5.5615\n",
      "Epoch 1, Batch 200, Loss: 4.3937\n",
      "Epoch 1, Batch 300, Loss: 4.0421\n",
      "Epoch 1, Batch 400, Loss: 3.5885\n",
      "Epoch 1, Batch 500, Loss: 3.4831\n",
      "Epoch 1, Batch 600, Loss: 3.3184\n",
      "Epoch 1, Batch 700, Loss: 3.2801\n",
      "Epoch 1, Batch 800, Loss: 2.9866\n",
      "Epoch 1, Batch 900, Loss: 2.6947\n",
      "Epoch 1, Batch 1000, Loss: 2.6896\n",
      "Epoch 1, Batch 1100, Loss: 2.5803\n",
      "Epoch 1, Batch 1200, Loss: 2.5494\n",
      "Epoch 1, Batch 1300, Loss: 2.4870\n",
      "Epoch 2, Batch 100, Loss: 2.3540\n",
      "Epoch 2, Batch 200, Loss: 2.3065\n",
      "Epoch 2, Batch 300, Loss: 2.1561\n",
      "Epoch 2, Batch 400, Loss: 2.2617\n",
      "Epoch 2, Batch 500, Loss: 2.2345\n",
      "Epoch 2, Batch 600, Loss: 2.1518\n",
      "Epoch 2, Batch 700, Loss: 2.0082\n",
      "Epoch 2, Batch 800, Loss: 2.1596\n",
      "Epoch 2, Batch 900, Loss: 2.0502\n",
      "Epoch 2, Batch 1000, Loss: 2.0298\n",
      "Epoch 2, Batch 1100, Loss: 1.9870\n",
      "Epoch 2, Batch 1200, Loss: 2.0314\n",
      "Epoch 2, Batch 1300, Loss: 1.9144\n",
      "Epoch 3, Batch 100, Loss: 1.9181\n",
      "Epoch 3, Batch 200, Loss: 1.8088\n",
      "Epoch 3, Batch 300, Loss: 1.7666\n",
      "Epoch 3, Batch 400, Loss: 1.8172\n",
      "Epoch 3, Batch 500, Loss: 1.8235\n",
      "Epoch 3, Batch 600, Loss: 1.9908\n",
      "Epoch 3, Batch 700, Loss: 1.7007\n",
      "Epoch 3, Batch 800, Loss: 1.7646\n",
      "Epoch 3, Batch 900, Loss: 1.6832\n",
      "Epoch 3, Batch 1000, Loss: 1.7359\n",
      "Epoch 3, Batch 1100, Loss: 1.7746\n",
      "Epoch 3, Batch 1200, Loss: 1.6877\n",
      "Epoch 3, Batch 1300, Loss: 1.8383\n",
      "Epoch 4, Batch 100, Loss: 1.6579\n",
      "Epoch 4, Batch 200, Loss: 1.5877\n",
      "Epoch 4, Batch 300, Loss: 1.6192\n",
      "Epoch 4, Batch 400, Loss: 1.5955\n",
      "Epoch 4, Batch 500, Loss: 1.6418\n",
      "Epoch 4, Batch 600, Loss: 1.5946\n",
      "Epoch 4, Batch 700, Loss: 1.4727\n",
      "Epoch 4, Batch 800, Loss: 1.6887\n",
      "Epoch 4, Batch 900, Loss: 1.6797\n",
      "Epoch 4, Batch 1000, Loss: 1.6460\n",
      "Epoch 4, Batch 1100, Loss: 1.6464\n",
      "Epoch 4, Batch 1200, Loss: 1.6571\n",
      "Epoch 4, Batch 1300, Loss: 1.6414\n",
      "Epoch 5, Batch 100, Loss: 1.4632\n",
      "Epoch 5, Batch 200, Loss: 1.5142\n",
      "Epoch 5, Batch 300, Loss: 1.4463\n",
      "Epoch 5, Batch 400, Loss: 1.5212\n",
      "Epoch 5, Batch 500, Loss: 1.4641\n",
      "Epoch 5, Batch 600, Loss: 1.5661\n",
      "Epoch 5, Batch 700, Loss: 1.4311\n",
      "Epoch 5, Batch 800, Loss: 1.4437\n",
      "Epoch 5, Batch 900, Loss: 1.4465\n",
      "Epoch 5, Batch 1000, Loss: 1.5549\n",
      "Epoch 5, Batch 1100, Loss: 1.5043\n",
      "Epoch 5, Batch 1200, Loss: 1.5212\n",
      "Epoch 5, Batch 1300, Loss: 1.4673\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 94.26%\n",
      "Accuracy of the network on the test images: 94.26%\n",
      "Precision of the network on the test images: 93.94%\n",
      "Recall of the network on the test images: 94.62%\n",
      "Specificity of the network on the test images: 94.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CIFAKEDataset...\")\n",
    "dataset_1 = CIFAKEDataset(\"data/CIFAKE\", num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(dataset_1))\n",
    "print(\"Data dimension:\", dataset_1.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "dataset_1.show_example(0)\n",
    "\n",
    "# Instantiate the model\n",
    "model_1 = CIFAKEClassifier()\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_loader_1, test_loader_1 = make_dataloader(dataset_1)\n",
    "train_model(model_1, train_loader_1, epochs)\n",
    "test_model(model_1, test_loader_1)\n",
    "test_model_with_metrics(model_1, test_loader_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaa242-b055-4e6b-a6bc-e60a6cf033e8",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Original CIFAKE Data Model on SD 2.1 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53c75c1-83b5-4488-b287-9fa304cd53bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:48<00:00, 260.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_dogs_with_modifiers: 98.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:45<00:00, 271.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 98.168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_dogs_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6170f9fd-dd1b-414b-8b53-7586c15e7ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:35<00:00, 281.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with original fake images 94.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:35<00:00, 283.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test the model with original real images 4.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [01:11<00:00, 175.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/Cat\\source.txt, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\Cat\\\\source.txt'\n",
      "Test the model with real images online 3.760300824065925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [04:38<00:00, 44.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images with exact prompt 74.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:41<00:00, 304.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 jpg images with exact prompt 98.168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images without exact prompt 49.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [00:46<00:00, 266.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/CIFAKE-2/train/FAKE\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\CIFAKE-2\\\\train\\\\FAKE\\\\test.py'\n",
      "Test the model with 2.1 jpg images without exact prompt 89.44800000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Test the model with original fake images\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE/test/FAKE\")\n",
    "print(f\"Test the model with original fake images {s}\")\n",
    "# Step 2: Test the model with original real images\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE/test/REAL\")\n",
    "print(f\" Test the model with original real images {s}\")\n",
    "# Step 3: Test the model with real images online\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/Cat\")\n",
    "print(f\"Test the model with real images online {s}\")\n",
    "# Step 4: Test the model with 2.1 png images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Test the model with 2.1 png images with exact prompt {s}\")\n",
    "# Step 5: Test the model with 2.1 jpg images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Test the model with 2.1 jpg images with exact prompt {s}\")\n",
    "# Step 6: Test the model with 2.1 png images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats\")\n",
    "print(f\"Test the model with 2.1 png images without exact prompt {s}\")\n",
    "# Step 7: Test the model with 2.1 jpg images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE-2/train/FAKE\")\n",
    "print(f\"Test the model with 2.1 jpg images without exact prompt {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030a14b-6a83-4555-aa2c-d3b0bf64ea26",
   "metadata": {},
   "source": [
    "## Step 6: Load 2.1 CIFAKE Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4626002-355e-47f5-910b-c2021b5e4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset 2.1...\n",
      "Loading folder: data/CIFAKE-3\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e56ce1bdc34156b5b30c89bd0dbc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb2200fe8544662981e9b307239c974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ffd306ca6447559a2cc3e0229456dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6abbe6b0810403aa8fedf37b604728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 49998\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 5.2348\n",
      "Epoch 1, Batch 200, Loss: 3.0914\n",
      "Epoch 1, Batch 300, Loss: 2.7178\n",
      "Epoch 1, Batch 400, Loss: 2.5538\n",
      "Epoch 1, Batch 500, Loss: 2.2874\n",
      "Epoch 2, Batch 100, Loss: 2.0899\n",
      "Epoch 2, Batch 200, Loss: 1.9109\n",
      "Epoch 2, Batch 300, Loss: 1.9061\n",
      "Epoch 2, Batch 400, Loss: 1.8811\n",
      "Epoch 2, Batch 500, Loss: 1.7420\n",
      "Epoch 3, Batch 100, Loss: 1.6129\n",
      "Epoch 3, Batch 200, Loss: 1.5789\n",
      "Epoch 3, Batch 300, Loss: 1.4895\n",
      "Epoch 3, Batch 400, Loss: 1.3946\n",
      "Epoch 3, Batch 500, Loss: 1.3793\n",
      "Epoch 4, Batch 100, Loss: 1.1775\n",
      "Epoch 4, Batch 200, Loss: 1.2011\n",
      "Epoch 4, Batch 300, Loss: 1.2408\n",
      "Epoch 4, Batch 400, Loss: 1.2837\n",
      "Epoch 4, Batch 500, Loss: 1.2815\n",
      "Epoch 5, Batch 100, Loss: 1.0691\n",
      "Epoch 5, Batch 200, Loss: 1.0501\n",
      "Epoch 5, Batch 300, Loss: 1.1007\n",
      "Epoch 5, Batch 400, Loss: 0.9841\n",
      "Epoch 5, Batch 500, Loss: 1.0175\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 95.75%\n",
      "Accuracy of the network on the test images: 95.75%\n",
      "Precision of the network on the test images: 94.91%\n",
      "Recall of the network on the test images: 96.75%\n",
      "Specificity of the network on the test images: 96.65%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CIFAKEDataset 2.1...\")\n",
    "dataset_2 = CIFAKEDataset(\"data/CIFAKE-3\", num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(dataset_2))\n",
    "print(\"Data dimension:\", dataset_2.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "dataset_2.show_example(0)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = CIFAKEClassifier()\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_loader_2, test_loader_2 = make_dataloader(dataset_2)\n",
    "train_model(model_2, train_loader_2, epochs)\n",
    "test_model(model_2, test_loader_2)\n",
    "test_model_with_metrics(model_2, test_loader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a29c5-6952-408f-b376-f9618eded22f",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Original CIFAKE Data Model on SD 2.1 Images Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016d1668-9d12-4019-a33c-8d999cb62a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [05:53<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_dogs_with_modifiers\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_dogs_with_modifiers\\\\test.py'\n",
      "Specificity on data/sd_2_1_dogs_with_modifiers: 77.78399999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [05:34<00:00, 37.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 63.624\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:47<00:00, 265.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_dogs_with_modifiers: 96.16799999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:44<00:00, 279.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 94.16799999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_dogs_with_modifiers\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")\n",
    "print(\"=====================\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_dogs_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3916cd22-47d6-41dd-b7da-9a608f6dd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:06<00:00, 1443.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with original fake images 35.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:06<00:00, 1457.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test the model with original real images 1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:23<00:00, 535.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/Cat\\source.txt, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\Cat\\\\source.txt'\n",
      "Test the model with real images online 0.23201856148491878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [03:11<00:00, 65.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images with exact prompt 63.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:08<00:00, 1508.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 jpg images with exact prompt 94.16799999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 72.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images without exact prompt 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [00:08<00:00, 1540.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/CIFAKE-2/train/FAKE\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\CIFAKE-2\\\\train\\\\FAKE\\\\test.py'\n",
      "Test the model with 2.1 jpg images without exact prompt 81.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Test the model with original fake images\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE/test/FAKE\")\n",
    "print(f\"Test the model with original fake images {s}\")\n",
    "# Step 2: Test the model with original real images\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE/test/REAL\")\n",
    "print(f\" Test the model with original real images {s}\")\n",
    "# Step 3: Test the model with real images online\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/Cat\")\n",
    "print(f\"Test the model with real images online {s}\")\n",
    "# Step 4: Test the model with 2.1 png images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Test the model with 2.1 png images with exact prompt {s}\")\n",
    "# Step 5: Test the model with 2.1 jpg images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Test the model with 2.1 jpg images with exact prompt {s}\")\n",
    "# Step 6: Test the model with 2.1 png images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats\")\n",
    "print(f\"Test the model with 2.1 png images without exact prompt {s}\")\n",
    "# Step 7: Test the model with 2.1 jpg images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE-2/train/FAKE\")\n",
    "print(f\"Test the model with 2.1 jpg images without exact prompt {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aec462-e2cf-4269-bc90-badc8bac78ff",
   "metadata": {},
   "source": [
    "## Step 8: Find Top 200 Most Fake 2.1 Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3ed71f-6a50-46db-8af4-575fe591d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/sd_2_1_dogs_with_modifiers_small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8363bbdce006478aa6a3d7834359ae5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3154.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10544_n.jpg', 'score': 1.705892032077827e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11909_n.jpg', 'score': 2.510305876057828e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11011_n.jpg', 'score': 2.8867320907011162e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7130_n.jpg', 'score': 3.357299874551245e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10139_n.jpg', 'score': 4.334963250585133e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3463_n.jpg', 'score': 4.657059889723314e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_507_n.jpg', 'score': 5.331542070052819e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1112_n.jpg', 'score': 5.820264050271362e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4630_n.jpg', 'score': 6.05514060225687e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1511_n.jpg', 'score': 6.105250577093102e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11107_n.jpg', 'score': 6.185428901517298e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_832_n.jpg', 'score': 6.831086011516163e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5877_n.jpg', 'score': 8.402477760682814e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1951_n.jpg', 'score': 8.48228137328988e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9866_n.jpg', 'score': 9.127818884735461e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7025_n.jpg', 'score': 9.538857739244122e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_853_n.jpg', 'score': 9.888020940707065e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6043_n.jpg', 'score': 1.0107902198797092e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5373_n.jpg', 'score': 1.0591338650556281e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6558_n.jpg', 'score': 1.0629090866132174e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2839_n.jpg', 'score': 1.0952850971079897e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6964_n.jpg', 'score': 1.0958753591694403e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2109_n.jpg', 'score': 1.10845367089496e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7337_n.jpg', 'score': 1.1448856639617588e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6053_n.jpg', 'score': 1.184020220534876e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2496_n.jpg', 'score': 1.1864798580063507e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3963_n.jpg', 'score': 1.2040210094710346e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4582_n.jpg', 'score': 1.2439840247679967e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8180_n.jpg', 'score': 1.2524738849606365e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8728_n.jpg', 'score': 1.4407585695153102e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4019_n.jpg', 'score': 1.4732700037711766e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1606_n.jpg', 'score': 1.621440242161043e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7097_n.jpg', 'score': 1.630665428820066e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10590_n.jpg', 'score': 1.6476682503707707e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9023_n.jpg', 'score': 1.656557469686959e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4401_n.jpg', 'score': 1.6734582459321246e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_821_n.jpg', 'score': 1.7226528143510222e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3741_n.jpg', 'score': 1.760947452567052e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6898_n.jpg', 'score': 1.7818847481976263e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9168_n.jpg', 'score': 1.810062167351134e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7220_n.jpg', 'score': 1.832558336900547e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4114_n.jpg', 'score': 1.876775968412403e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8427_n.jpg', 'score': 1.940204128914047e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12179_n.jpg', 'score': 1.9904118744307198e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1035_n.jpg', 'score': 2.0010489606647752e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1406_n.jpg', 'score': 2.0113018763368018e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3041_n.jpg', 'score': 2.0399687855388038e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11522_n.jpg', 'score': 2.0564719307003543e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2846_n.jpg', 'score': 2.0602446966222487e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2031_n.jpg', 'score': 2.0990542907384224e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6248_n.jpg', 'score': 2.1083831597934477e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4799_n.jpg', 'score': 2.118329211953096e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3774_n.jpg', 'score': 2.1251378711895086e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6069_n.jpg', 'score': 2.130572102032602e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6186_n.jpg', 'score': 2.2133350285002962e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4165_n.jpg', 'score': 2.2136306142783724e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6385_n.jpg', 'score': 2.237934313598089e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2834_n.jpg', 'score': 2.2564545361092314e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2595_n.jpg', 'score': 2.269777178298682e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5951_n.jpg', 'score': 2.3289241653401405e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7607_n.jpg', 'score': 2.334024611627683e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9283_n.jpg', 'score': 2.372905692027416e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2002_n.jpg', 'score': 2.3782702555763535e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_768_n.jpg', 'score': 2.4184670110116713e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9112_n.jpg', 'score': 2.4398346795351245e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5040_n.jpg', 'score': 2.5358609491377138e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9432_n.jpg', 'score': 2.5692337658256292e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5959_n.jpg', 'score': 2.576481711002998e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7961_n.jpg', 'score': 2.6729947421699762e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_842_n.jpg', 'score': 2.7419844627729617e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9189_n.jpg', 'score': 2.7570427846512757e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2590_n.jpg', 'score': 2.7960928491665982e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9211_n.jpg', 'score': 2.8388214559527114e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8701_n.jpg', 'score': 2.856767241610214e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8616_n.jpg', 'score': 2.860196582332719e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10587_n.jpg', 'score': 3.143136200378649e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12235_n.jpg', 'score': 3.1529500120086595e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9692_n.jpg', 'score': 3.2113257475430146e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12135_n.jpg', 'score': 3.251934322179295e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3975_n.jpg', 'score': 3.287302024546079e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6755_n.jpg', 'score': 3.2981319236569107e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7205_n.jpg', 'score': 3.372236460563727e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12275_n.jpg', 'score': 3.435618418734521e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10858_n.jpg', 'score': 3.4470554965082556e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1341_n.jpg', 'score': 3.460906373220496e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4263_n.jpg', 'score': 3.501546962070279e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1968_n.jpg', 'score': 3.517706136335619e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8918_n.jpg', 'score': 3.632908556028269e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9907_n.jpg', 'score': 3.662067683762871e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7988_n.jpg', 'score': 3.719401502166875e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7891_n.jpg', 'score': 3.72124595742207e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10599_n.jpg', 'score': 3.803427534876391e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6008_n.jpg', 'score': 3.818786717602052e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6100_n.jpg', 'score': 3.840040517388843e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11202_n.jpg', 'score': 3.887753700837493e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1418_n.jpg', 'score': 3.908660801243968e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7591_n.jpg', 'score': 3.9567814383190125e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8957_n.jpg', 'score': 4.0406830521533266e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2783_n.jpg', 'score': 4.1241532017011195e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_42_n.jpg', 'score': 4.176965376245789e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11937_n.jpg', 'score': 4.1876501200022176e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3513_n.jpg', 'score': 4.2406409193063155e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3192_n.jpg', 'score': 4.2571929952828214e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4085_n.jpg', 'score': 4.27312079409603e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1630_n.jpg', 'score': 4.289161370252259e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5027_n.jpg', 'score': 4.3572184949880466e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9184_n.jpg', 'score': 4.41675801994279e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_285_n.jpg', 'score': 4.435160371940583e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3203_n.jpg', 'score': 4.4486219849204645e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10764_n.jpg', 'score': 4.45032783318311e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6842_n.jpg', 'score': 4.57642090623267e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9054_n.jpg', 'score': 4.648955655284226e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11629_n.jpg', 'score': 4.660879130824469e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5632_n.jpg', 'score': 4.672022259910591e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2825_n.jpg', 'score': 4.760477168019861e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3613_n.jpg', 'score': 4.823677591048181e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6363_n.jpg', 'score': 4.82914547319524e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_826_n.jpg', 'score': 4.878831532550976e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6491_n.jpg', 'score': 4.9016918637789786e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11394_n.jpg', 'score': 4.995938797947019e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7507_n.jpg', 'score': 5.018692172598094e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1918_n.jpg', 'score': 5.033588968217373e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6525_n.jpg', 'score': 5.082673305878416e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11569_n.jpg', 'score': 5.084980875835754e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9150_n.jpg', 'score': 5.089046317152679e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1624_n.jpg', 'score': 5.1018159865634516e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10801_n.jpg', 'score': 5.109840276418254e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7606_n.jpg', 'score': 5.134355524205603e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4413_n.jpg', 'score': 5.2526196668623015e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4694_n.jpg', 'score': 5.263736238703132e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_812_n.jpg', 'score': 5.266482912702486e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2382_n.jpg', 'score': 5.269874236546457e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_695_n.jpg', 'score': 5.281928679323755e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12368_n.jpg', 'score': 5.283651626086794e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3870_n.jpg', 'score': 5.349520506570116e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3644_n.jpg', 'score': 5.390539445215836e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10106_n.jpg', 'score': 5.4157870181370527e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10251_n.jpg', 'score': 5.446458453661762e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_195_n.jpg', 'score': 5.4615051340078935e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4403_n.jpg', 'score': 5.474013596540317e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3930_n.jpg', 'score': 5.598568532150239e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10841_n.jpg', 'score': 5.6137887440854684e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3064_n.jpg', 'score': 5.6564567785244435e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1275_n.jpg', 'score': 5.7229110097978264e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1577_n.jpg', 'score': 5.7583300076657906e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3984_n.jpg', 'score': 5.785499160992913e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1488_n.jpg', 'score': 5.789693386759609e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8625_n.jpg', 'score': 5.8271423768019304e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7559_n.jpg', 'score': 5.877958028577268e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9322_n.jpg', 'score': 5.926870653638616e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6920_n.jpg', 'score': 5.939431139267981e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4336_n.jpg', 'score': 6.00790954194963e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3720_n.jpg', 'score': 6.011778168613091e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4023_n.jpg', 'score': 6.0138703702250496e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1835_n.jpg', 'score': 6.0423546528909355e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1592_n.jpg', 'score': 6.0651462263194844e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8724_n.jpg', 'score': 6.094366835895926e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9699_n.jpg', 'score': 6.19205748080276e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1400_n.jpg', 'score': 6.216871406650171e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_454_n.jpg', 'score': 6.225710967555642e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7362_n.jpg', 'score': 6.287366704782471e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11841_n.jpg', 'score': 6.288014265010133e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6557_n.jpg', 'score': 6.296378705883399e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3456_n.jpg', 'score': 6.363611464621499e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8476_n.jpg', 'score': 6.365917943185195e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11594_n.jpg', 'score': 6.527270306833088e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2310_n.jpg', 'score': 6.588405085494742e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7474_n.jpg', 'score': 6.60648729535751e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6789_n.jpg', 'score': 6.618833867833018e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11618_n.jpg', 'score': 6.642304651904851e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3821_n.jpg', 'score': 6.714837945764884e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6303_n.jpg', 'score': 6.756879156455398e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10227_n.jpg', 'score': 6.800097617087886e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5283_n.jpg', 'score': 6.819982081651688e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10324_n.jpg', 'score': 6.844362360425293e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_332_n.jpg', 'score': 6.903152097947896e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6642_n.jpg', 'score': 6.967614899622276e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8586_n.jpg', 'score': 7.073600136209279e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5321_n.jpg', 'score': 7.184930291259661e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3399_n.jpg', 'score': 7.240538980113342e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4359_n.jpg', 'score': 7.349294901359826e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_646_n.jpg', 'score': 7.353725231951103e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10940_n.jpg', 'score': 7.360930612776428e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7864_n.jpg', 'score': 7.376017310889438e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9863_n.jpg', 'score': 7.409509998979047e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1393_n.jpg', 'score': 7.416119478875771e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_390_n.jpg', 'score': 7.418022869387642e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12218_n.jpg', 'score': 7.432630081893876e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10161_n.jpg', 'score': 7.476362225133926e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5451_n.jpg', 'score': 7.477617327822372e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3439_n.jpg', 'score': 7.522345549659804e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10150_n.jpg', 'score': 7.562104292446747e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10574_n.jpg', 'score': 7.607410952914506e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8402_n.jpg', 'score': 7.607998122693971e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3539_n.jpg', 'score': 7.637933595106006e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7492_n.jpg', 'score': 7.645657751709223e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1215_n.jpg', 'score': 7.655828085262328e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2790_n.jpg', 'score': 7.79109905124642e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10443_n.jpg', 'score': 7.791270036250353e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9119_n.jpg', 'score': 7.854657451389357e-05}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_3 = CIFAKEDataset(\"data/sd_2_1_dogs_with_modifiers_small\", num_processes=4, use_cifake_fold_structure=False)\n",
    "\n",
    "results = []\n",
    "for x, y in tqdm(dataset_3, desc=\"Evaluating images\"):\n",
    "    x = x.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model_2(x).squeeze().item()\n",
    "        file = dataset_3.latest_file_path\n",
    "        results.append({\"file\": file, \"score\": output})\n",
    "\n",
    "# Sort the list of dictionaries by the \"score\" key\n",
    "sorted_results = sorted(results, key=lambda x: x['score'])\n",
    "most_fakes = sorted_results[:200]\n",
    "print(most_fakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cc708-6941-4036-83ef-da83a39d7524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
