{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c181e285-4131-4c59-8094-6c510c17388e",
   "metadata": {},
   "source": [
    "## Step 1: Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e59f121-586b-48a2-b502-d25d44ef92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import fnmatch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def list_files_by_type(folder_path, file_type):\n",
    "    filtered_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if fnmatch.fnmatch(file, f\"*.{file_type}\"):\n",
    "            filtered_files.append(os.path.join(folder_path, file))\n",
    "    return filtered_files\n",
    "\n",
    "def process_image(args):\n",
    "    file_path, label, transform = args  # Unpack the tuple\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    default_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    tensor = default_transform(image)\n",
    "    if transform:\n",
    "        tensor = transform(tensor)\n",
    "    return tensor, torch.tensor(label, dtype=torch.long), file_path\n",
    "\n",
    "class CIFAKEDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def extract_index_and_category(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        pattern = r\"(\\d+)(?: \\((\\d+)\\))?\\..+\"\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            category = int(match.group(2)) if match.group(2) else 0\n",
    "            return index, category\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_folder(folder_path, label, category=None, transform=None, num_processes=1):\n",
    "        print(f\"Loading folder: {folder_path}\")\n",
    "        files = list_files_by_type(folder_path, \"jpg\")\n",
    "        if category is not None:\n",
    "            files = [file for file in files if CIFAKEDataset.extract_index_and_category(file)[1] == category]\n",
    "\n",
    "        # Use process_map from tqdm.contrib.concurrent for better tqdm updates\n",
    "        results = thread_map(process_image, [(file, label, transform) for file in files], max_workers=num_processes, chunksize=1)\n",
    "\n",
    "        x = torch.stack([result[0] for result in results])\n",
    "        y = torch.stack([result[1] for result in results])\n",
    "        file_paths = [result[2] for result in results]\n",
    "        return x, y, file_paths\n",
    "        \n",
    "    def __init__(self, folder_path, category=None, transform=None, num_processes=1, use_cifake_fold_structure=True):\n",
    "        self.latest_file_path = None\n",
    "        \n",
    "        if use_cifake_fold_structure:\n",
    "            label_1_folders = [\n",
    "                os.path.join(folder_path, \"train/REAL\"),\n",
    "                os.path.join(folder_path, \"test/REAL\"),\n",
    "            ]\n",
    "            label_0_folders = [\n",
    "                os.path.join(folder_path, \"train/FAKE\"),\n",
    "                os.path.join(folder_path, \"test/FAKE\"),\n",
    "            ]\n",
    "            x1, y1, fp_1 = CIFAKEDataset.load_folder(label_1_folders[0], 1, category, transform, num_processes)\n",
    "            x2, y2, fp_2 = CIFAKEDataset.load_folder(label_0_folders[0], 0, category, transform, num_processes)\n",
    "            x3, y3, fp_3 = CIFAKEDataset.load_folder(label_1_folders[1], 1, category, transform, num_processes)\n",
    "            x4, y4, fp_4 = CIFAKEDataset.load_folder(label_0_folders[1], 0, category, transform, num_processes)\n",
    "            self.x = torch.cat((x1, x2, x3, x4))\n",
    "            self.y = torch.cat((y1, y2, y3, y4))\n",
    "            self.file_paths = fp_1 + fp_2 + fp_3 + fp_4\n",
    "        else:\n",
    "            self.x, self.y, self.file_paths = CIFAKEDataset.load_folder(folder_path, 0, None, transform, num_processes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.latest_file_path = self.file_paths[idx]\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def data_dim(self):\n",
    "        return self.x[0].size()\n",
    "    \n",
    "    def show_example(self, idx):\n",
    "        x, y = self[idx]\n",
    "        image_array = x.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(f\"Label: {y}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def latest_file_path(self):\n",
    "        return self.latest_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d5a3a-e904-4b5b-ba0a-aa591661adda",
   "metadata": {},
   "source": [
    "## Step 2: Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240f413-e894-4c54-bb83-edf9d3c385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network model following the architecture in the provided diagram\n",
    "class CIFAKEClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAKEClassifier, self).__init__()\n",
    "        # Assuming the input image size is 32x32x3 as per the rescale block in the diagram\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Convolutional layer with 32 outputs\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer with a 2x2 window and stride 2\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Second convolutional layer with 32 outputs\n",
    "        # Flatten layer will be applied in the forward pass\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 64)  # Dense layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, 1)  # Final dense layer with 1 unit for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU activation function after first convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU activation function after second convolution\n",
    "        x = self.pool(x)  # Apply max pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor for the dense layer\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first dense layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply sigmoid activation function for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fd1d9-1842-43cc-99e4-3408d3c6437b",
   "metadata": {},
   "source": [
    "## Step 3: Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc8a162-ba0e-4a3d-9bdf-685985441d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "def make_dataloader(dataset, training_data_ratio=0.7):\n",
    "    # Create the training and testing splits\n",
    "    train_size = int(training_data_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Dataloader for batch training\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, dataloader, epochs):\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()  # BCELoss expects labels to be in float format\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()  # Remove unnecessary dimensions\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# Test the model and calculate precision, recall, and specificity\n",
    "def test_model_with_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(outputs)  # Round to get binary predictions\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    precision = 100 * precision_score(all_labels, all_predictions)\n",
    "    recall = 100 * recall_score(all_labels, all_predictions)\n",
    "\n",
    "    # Calculate specificity\n",
    "    TN = ((1 - np.array(all_predictions)) * (1 - np.array(all_labels))).sum()\n",
    "    FN = ((1 - np.array(all_predictions)) * np.array(all_labels)).sum()\n",
    "    specificity = 100 * TN / (TN + FN)\n",
    "\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    print(f'Precision of the network on the test images: {precision:.2f}%')\n",
    "    print(f'Recall of the network on the test images: {recall:.2f}%')\n",
    "    print(f'Specificity of the network on the test images: {specificity:.2f}%')\n",
    "\n",
    "def evaluate_folder(model, folder_path):\n",
    "    \"\"\"Evaluate a single folder of images and return the counts and rates.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "    total_images = 0\n",
    "\n",
    "    image_list = os.listdir(folder_path)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "    ])\n",
    "    for image_name in tqdm(image_list, desc=\"Evaluating images\"):\n",
    "        try:\n",
    "            image_path = os.path.join(folder_path, image_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to load one image {image_path}, Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image).squeeze()  # Remove unnecessary dimensions\n",
    "            predicted = torch.round(output)  # Round to get binary predictions\n",
    "            label = predicted.item()\n",
    "\n",
    "        if label == 0:\n",
    "            count_0 += 1\n",
    "        else:\n",
    "            count_1 += 1\n",
    "\n",
    "        total_images += 1\n",
    "\n",
    "    rate_0 = (count_0 / total_images) * 100 if total_images > 0 else 0\n",
    "    rate_1 = (count_1 / total_images) * 100 if total_images > 0 else 0\n",
    "\n",
    "    return count_0, count_1, rate_0, rate_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e1329-3d21-4c27-a3db-89dc5df4b33e",
   "metadata": {},
   "source": [
    "## Step 4: Load Original CIFAKE Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba49d85-a39b-438a-a695-f1ec5e4de58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset...\n",
      "Loading folder: data/CIFAKE\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0909fd4f890e4a5eaf83b481127940ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bff19dfe5b4190a1c61d3e840e2689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d628bc32444b1c94f91de1ffe79b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74937f47e94397b3debe243c3d6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 120000\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 5.5278\n",
      "Epoch 1, Batch 200, Loss: 4.3569\n",
      "Epoch 1, Batch 300, Loss: 4.1095\n",
      "Epoch 1, Batch 400, Loss: 3.8404\n",
      "Epoch 1, Batch 500, Loss: 3.6380\n",
      "Epoch 1, Batch 600, Loss: 3.1540\n",
      "Epoch 1, Batch 700, Loss: 3.1836\n",
      "Epoch 1, Batch 800, Loss: 2.9912\n",
      "Epoch 1, Batch 900, Loss: 2.9545\n",
      "Epoch 1, Batch 1000, Loss: 2.8177\n",
      "Epoch 1, Batch 1100, Loss: 2.5889\n",
      "Epoch 1, Batch 1200, Loss: 2.7327\n",
      "Epoch 1, Batch 1300, Loss: 2.4844\n",
      "Epoch 2, Batch 100, Loss: 2.3636\n",
      "Epoch 2, Batch 200, Loss: 2.4212\n",
      "Epoch 2, Batch 300, Loss: 2.2612\n",
      "Epoch 2, Batch 400, Loss: 2.4744\n",
      "Epoch 2, Batch 500, Loss: 2.2679\n",
      "Epoch 2, Batch 600, Loss: 2.2638\n",
      "Epoch 2, Batch 700, Loss: 2.2199\n",
      "Epoch 2, Batch 800, Loss: 2.1197\n",
      "Epoch 2, Batch 900, Loss: 2.1454\n",
      "Epoch 2, Batch 1000, Loss: 2.0386\n",
      "Epoch 2, Batch 1100, Loss: 2.0898\n",
      "Epoch 2, Batch 1200, Loss: 2.1680\n",
      "Epoch 2, Batch 1300, Loss: 2.0680\n",
      "Epoch 3, Batch 100, Loss: 1.8663\n",
      "Epoch 3, Batch 200, Loss: 1.8879\n",
      "Epoch 3, Batch 300, Loss: 1.8285\n",
      "Epoch 3, Batch 400, Loss: 1.8311\n",
      "Epoch 3, Batch 500, Loss: 1.8219\n",
      "Epoch 3, Batch 600, Loss: 1.8639\n",
      "Epoch 3, Batch 700, Loss: 1.8493\n",
      "Epoch 3, Batch 800, Loss: 1.8516\n",
      "Epoch 3, Batch 900, Loss: 1.9545\n",
      "Epoch 3, Batch 1000, Loss: 1.9011\n",
      "Epoch 3, Batch 1100, Loss: 1.8580\n",
      "Epoch 3, Batch 1200, Loss: 1.7702\n",
      "Epoch 3, Batch 1300, Loss: 1.9703\n",
      "Epoch 4, Batch 100, Loss: 1.6519\n",
      "Epoch 4, Batch 200, Loss: 1.7483\n",
      "Epoch 4, Batch 300, Loss: 1.7883\n",
      "Epoch 4, Batch 400, Loss: 1.6711\n",
      "Epoch 4, Batch 500, Loss: 1.6532\n",
      "Epoch 4, Batch 600, Loss: 1.5705\n",
      "Epoch 4, Batch 700, Loss: 1.6094\n",
      "Epoch 4, Batch 800, Loss: 1.7655\n",
      "Epoch 4, Batch 900, Loss: 1.6150\n",
      "Epoch 4, Batch 1000, Loss: 1.7568\n",
      "Epoch 4, Batch 1100, Loss: 1.7610\n",
      "Epoch 4, Batch 1200, Loss: 1.6392\n",
      "Epoch 4, Batch 1300, Loss: 1.6375\n",
      "Epoch 5, Batch 100, Loss: 1.5123\n",
      "Epoch 5, Batch 200, Loss: 1.5771\n",
      "Epoch 5, Batch 300, Loss: 1.6242\n",
      "Epoch 5, Batch 400, Loss: 1.4288\n",
      "Epoch 5, Batch 500, Loss: 1.7375\n",
      "Epoch 5, Batch 600, Loss: 1.5736\n",
      "Epoch 5, Batch 700, Loss: 1.4532\n",
      "Epoch 5, Batch 800, Loss: 1.6365\n",
      "Epoch 5, Batch 900, Loss: 1.5611\n",
      "Epoch 5, Batch 1000, Loss: 1.5164\n",
      "Epoch 5, Batch 1100, Loss: 1.5205\n",
      "Epoch 5, Batch 1200, Loss: 1.6416\n",
      "Epoch 5, Batch 1300, Loss: 1.4829\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 93.57%\n",
      "Accuracy of the network on the test images: 93.57%\n",
      "Precision of the network on the test images: 91.73%\n",
      "Recall of the network on the test images: 95.82%\n",
      "Specificity of the network on the test images: 95.60%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CIFAKEDataset...\")\n",
    "dataset_1 = CIFAKEDataset(\"data/CIFAKE\", num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(dataset_1))\n",
    "print(\"Data dimension:\", dataset_1.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "dataset_1.show_example(0)\n",
    "\n",
    "# Instantiate the model\n",
    "model_1 = CIFAKEClassifier()\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_loader_1, test_loader_1 = make_dataloader(dataset_1)\n",
    "train_model(model_1, train_loader_1, epochs)\n",
    "test_model(model_1, test_loader_1)\n",
    "test_model_with_metrics(model_1, test_loader_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaa242-b055-4e6b-a6bc-e60a6cf033e8",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Original CIFAKE Data Model on SD 2.1 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53c75c1-83b5-4488-b287-9fa304cd53bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:46<00:00, 269.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_dogs_with_modifiers: 98.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:45<00:00, 273.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 98.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_dogs_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6170f9fd-dd1b-414b-8b53-7586c15e7ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:06<00:00, 1547.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with original fake images 91.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:06<00:00, 1566.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test the model with original real images 3.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [01:08<00:00, 181.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/Cat\\source.txt, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\Cat\\\\source.txt'\n",
      "Test the model with real images online 2.5682054564365147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [05:25<00:00, 38.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images with exact prompt 67.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:07<00:00, 1579.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 jpg images with exact prompt 98.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 37.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images without exact prompt 43.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [00:44<00:00, 279.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/CIFAKE-2/train/FAKE\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\CIFAKE-2\\\\train\\\\FAKE\\\\test.py'\n",
      "Test the model with 2.1 jpg images without exact prompt 92.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Test the model with original fake images\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE/test/FAKE\")\n",
    "print(f\"Test the model with original fake images {s}\")\n",
    "# Step 2: Test the model with original real images\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE/test/REAL\")\n",
    "print(f\" Test the model with original real images {s}\")\n",
    "# Step 3: Test the model with real images online\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/Cat\")\n",
    "print(f\"Test the model with real images online {s}\")\n",
    "# Step 4: Test the model with 2.1 png images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Test the model with 2.1 png images with exact prompt {s}\")\n",
    "# Step 5: Test the model with 2.1 jpg images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Test the model with 2.1 jpg images with exact prompt {s}\")\n",
    "# Step 6: Test the model with 2.1 png images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/sd_2_1_cats\")\n",
    "print(f\"Test the model with 2.1 png images without exact prompt {s}\")\n",
    "# Step 7: Test the model with 2.1 jpg images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_1, \"data/CIFAKE-2/train/FAKE\")\n",
    "print(f\"Test the model with 2.1 jpg images without exact prompt {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030a14b-6a83-4555-aa2c-d3b0bf64ea26",
   "metadata": {},
   "source": [
    "## Step 6: Load 2.1 CIFAKE Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4626002-355e-47f5-910b-c2021b5e4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAKEDataset 2.1...\n",
      "Loading folder: data/CIFAKE-3\\train/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc0842aa5fd42bf9060a934899e4f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\train/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af71c3a785b4e94b6cba7674abd40de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\test/REAL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe10231ace44c359ae2b4640169fc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/CIFAKE-3\\test/FAKE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295ac2d2e59a4e3f9e10ab7df9b17aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset length: 49998\n",
      "Data dimension: torch.Size([3, 32, 32])\n",
      "Showing example image...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 5.1398\n",
      "Epoch 1, Batch 200, Loss: 3.3619\n",
      "Epoch 1, Batch 300, Loss: 2.6336\n",
      "Epoch 1, Batch 400, Loss: 2.3388\n",
      "Epoch 1, Batch 500, Loss: 2.4992\n",
      "Epoch 2, Batch 100, Loss: 1.9740\n",
      "Epoch 2, Batch 200, Loss: 2.0064\n",
      "Epoch 2, Batch 300, Loss: 1.8376\n",
      "Epoch 2, Batch 400, Loss: 1.8187\n",
      "Epoch 2, Batch 500, Loss: 1.8466\n",
      "Epoch 3, Batch 100, Loss: 1.6760\n",
      "Epoch 3, Batch 200, Loss: 1.5481\n",
      "Epoch 3, Batch 300, Loss: 1.5228\n",
      "Epoch 3, Batch 400, Loss: 1.4138\n",
      "Epoch 3, Batch 500, Loss: 1.4274\n",
      "Epoch 4, Batch 100, Loss: 1.2466\n",
      "Epoch 4, Batch 200, Loss: 1.3552\n",
      "Epoch 4, Batch 300, Loss: 1.1168\n",
      "Epoch 4, Batch 400, Loss: 1.2556\n",
      "Epoch 4, Batch 500, Loss: 1.1680\n",
      "Epoch 5, Batch 100, Loss: 1.1113\n",
      "Epoch 5, Batch 200, Loss: 1.1392\n",
      "Epoch 5, Batch 300, Loss: 1.1001\n",
      "Epoch 5, Batch 400, Loss: 1.0992\n",
      "Epoch 5, Batch 500, Loss: 1.0453\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 94.61%\n",
      "Accuracy of the network on the test images: 94.61%\n",
      "Precision of the network on the test images: 92.27%\n",
      "Recall of the network on the test images: 97.40%\n",
      "Specificity of the network on the test images: 97.24%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CIFAKEDataset 2.1...\")\n",
    "dataset_2 = CIFAKEDataset(\"data/CIFAKE-3\", num_processes=4)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset length:\", len(dataset_2))\n",
    "print(\"Data dimension:\", dataset_2.data_dim())\n",
    "print(\"Showing example image...\")\n",
    "dataset_2.show_example(0)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = CIFAKEClassifier()\n",
    "\n",
    "# Adding training and testing to the notebook\n",
    "train_loader_2, test_loader_2 = make_dataloader(dataset_2)\n",
    "train_model(model_2, train_loader_2, epochs)\n",
    "test_model(model_2, test_loader_2)\n",
    "test_model_with_metrics(model_2, test_loader_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a29c5-6952-408f-b376-f9618eded22f",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Original CIFAKE Data Model on SD 2.1 Images Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016d1668-9d12-4019-a33c-8d999cb62a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [05:46<00:00, 36.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_dogs_with_modifiers\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_dogs_with_modifiers\\\\test.py'\n",
      "Specificity on data/sd_2_1_dogs_with_modifiers: 72.98400000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [04:39<00:00, 44.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 61.456\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:44<00:00, 282.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_dogs_with_modifiers: 93.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:40<00:00, 310.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity on data/sd_2_1_cats_with_modifiers: 91.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_dogs_with_modifiers\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")\n",
    "print(\"=====================\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_dogs_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_dogs_with_modifiers: {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Specificity on data/sd_2_1_cats_with_modifiers: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3916cd22-47d6-41dd-b7da-9a608f6dd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:32<00:00, 308.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with original fake images 31.419999999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:32<00:00, 311.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test the model with original real images 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [01:02<00:00, 198.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/Cat\\source.txt, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\Cat\\\\source.txt'\n",
      "Test the model with real images online 0.17601408112649014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [03:03<00:00, 68.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images with exact prompt 61.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:08<00:00, 1551.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 jpg images with exact prompt 91.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model with 2.1 png images without exact prompt 41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12501/12501 [00:42<00:00, 295.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/CIFAKE-2/train/FAKE\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\CIFAKE-2\\\\train\\\\FAKE\\\\test.py'\n",
      "Test the model with 2.1 jpg images without exact prompt 81.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Test the model with original fake images\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE/test/FAKE\")\n",
    "print(f\"Test the model with original fake images {s}\")\n",
    "# Step 2: Test the model with original real images\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE/test/REAL\")\n",
    "print(f\" Test the model with original real images {s}\")\n",
    "# Step 3: Test the model with real images online\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/Cat\")\n",
    "print(f\"Test the model with real images online {s}\")\n",
    "# Step 4: Test the model with 2.1 png images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers\")\n",
    "print(f\"Test the model with 2.1 png images with exact prompt {s}\")\n",
    "# Step 5: Test the model with 2.1 jpg images with exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats_with_modifiers_small\")\n",
    "print(f\"Test the model with 2.1 jpg images with exact prompt {s}\")\n",
    "# Step 6: Test the model with 2.1 png images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_cats\")\n",
    "print(f\"Test the model with 2.1 png images without exact prompt {s}\")\n",
    "# Step 7: Test the model with 2.1 jpg images without exact prompt\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/CIFAKE-2/train/FAKE\")\n",
    "print(f\"Test the model with 2.1 jpg images without exact prompt {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aec462-e2cf-4269-bc90-badc8bac78ff",
   "metadata": {},
   "source": [
    "## Step 8: Find Top 200 Most Fake 2.1 Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3ed71f-6a50-46db-8af4-575fe591d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: data/sd_2_1_dogs_with_modifiers_small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae548ceb74149d0b1be0d23efbec2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12500/12500 [00:03<00:00, 3255.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9112_n.jpg', 'score': 3.102124765064218e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1511_n.jpg', 'score': 4.293564870749833e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10162_n.jpg', 'score': 5.749614956584992e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6274_n.jpg', 'score': 6.886001756356563e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6842_n.jpg', 'score': 7.233850283228094e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6008_n.jpg', 'score': 9.13655458134599e-06}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9902_n.jpg', 'score': 1.0081149412144441e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6043_n.jpg', 'score': 1.0083649613079615e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3041_n.jpg', 'score': 1.3572131138062105e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10322_n.jpg', 'score': 1.4605133401346393e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2687_n.jpg', 'score': 1.5709762010374106e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11909_n.jpg', 'score': 1.621344381419476e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6623_n.jpg', 'score': 1.849797263275832e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3463_n.jpg', 'score': 1.8899574570241384e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2002_n.jpg', 'score': 1.9086277461610734e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_826_n.jpg', 'score': 1.999183587031439e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5917_n.jpg', 'score': 2.2510856069857255e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6248_n.jpg', 'score': 2.254701394122094e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6991_n.jpg', 'score': 2.315514939255081e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9944_n.jpg', 'score': 2.3182436052593403e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5830_n.jpg', 'score': 2.348542147956323e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5951_n.jpg', 'score': 2.370395122852642e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3253_n.jpg', 'score': 2.4421929992968217e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_695_n.jpg', 'score': 2.462201337039005e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10139_n.jpg', 'score': 2.5421853933949023e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10046_n.jpg', 'score': 2.5514218577882275e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4582_n.jpg', 'score': 2.5961711799027398e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2790_n.jpg', 'score': 2.684377432160545e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11269_n.jpg', 'score': 2.703401150938589e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6385_n.jpg', 'score': 2.7037747713620774e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_646_n.jpg', 'score': 2.755726018222049e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2429_n.jpg', 'score': 2.8224412744748406e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11202_n.jpg', 'score': 2.9403197913779877e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5820_n.jpg', 'score': 3.0198931199265644e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3321_n.jpg', 'score': 3.164547524647787e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7891_n.jpg', 'score': 3.26874251186382e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_574_n.jpg', 'score': 3.284632111899555e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4680_n.jpg', 'score': 3.290846143499948e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11937_n.jpg', 'score': 3.307505539851263e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3622_n.jpg', 'score': 3.318161907372996e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6103_n.jpg', 'score': 3.4047312510665506e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11778_n.jpg', 'score': 3.5460117942420766e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5050_n.jpg', 'score': 3.605727761168964e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11923_n.jpg', 'score': 3.6797042412217706e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9002_n.jpg', 'score': 3.703587208292447e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7502_n.jpg', 'score': 3.725308124558069e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11588_n.jpg', 'score': 3.879986979882233e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7736_n.jpg', 'score': 3.9291553548537195e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7988_n.jpg', 'score': 4.020718188257888e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1035_n.jpg', 'score': 4.1664388845674694e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3741_n.jpg', 'score': 4.274796083336696e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8202_n.jpg', 'score': 4.302274464862421e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7095_n.jpg', 'score': 4.343403270468116e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7174_n.jpg', 'score': 4.37368726124987e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1506_n.jpg', 'score': 4.3794760131277144e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1400_n.jpg', 'score': 4.449479092727415e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7690_n.jpg', 'score': 4.575085768010467e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_864_n.jpg', 'score': 4.5861157559556887e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7130_n.jpg', 'score': 4.603713023243472e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12135_n.jpg', 'score': 4.610720861819573e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5161_n.jpg', 'score': 4.63403484900482e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_832_n.jpg', 'score': 4.733104651677422e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8628_n.jpg', 'score': 4.775882916874252e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7961_n.jpg', 'score': 4.8119160055648535e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3459_n.jpg', 'score': 4.87122597405687e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2592_n.jpg', 'score': 4.8963429435389116e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5294_n.jpg', 'score': 4.960202204529196e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9622_n.jpg', 'score': 5.0786082283593714e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10938_n.jpg', 'score': 5.179806976229884e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3870_n.jpg', 'score': 5.199975203140639e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12341_n.jpg', 'score': 5.282865822664462e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5544_n.jpg', 'score': 5.343086741049774e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5321_n.jpg', 'score': 5.378154673962854e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10940_n.jpg', 'score': 5.43132409802638e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3064_n.jpg', 'score': 5.504536966327578e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3192_n.jpg', 'score': 5.5786418670322746e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2525_n.jpg', 'score': 5.597463677986525e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3512_n.jpg', 'score': 5.7043802371481434e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10060_n.jpg', 'score': 5.7426903367741033e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3930_n.jpg', 'score': 5.781130676041357e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4165_n.jpg', 'score': 5.805383625556715e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7122_n.jpg', 'score': 5.806225090054795e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2595_n.jpg', 'score': 5.829376823385246e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11162_n.jpg', 'score': 5.8402267313795164e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10101_n.jpg', 'score': 5.855021299794316e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7858_n.jpg', 'score': 5.875895658391528e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8918_n.jpg', 'score': 5.933453212492168e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1502_n.jpg', 'score': 5.96312565903645e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7405_n.jpg', 'score': 6.008637137711048e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_612_n.jpg', 'score': 6.060734449420124e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4565_n.jpg', 'score': 6.09206581430044e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9136_n.jpg', 'score': 6.132236740086228e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11874_n.jpg', 'score': 6.15121316513978e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6310_n.jpg', 'score': 6.17370824329555e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11011_n.jpg', 'score': 6.18535268586129e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9699_n.jpg', 'score': 6.21301369392313e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4943_n.jpg', 'score': 6.278805085457861e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6766_n.jpg', 'score': 6.287444557528943e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10291_n.jpg', 'score': 6.30602880846709e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10852_n.jpg', 'score': 6.337062950478867e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8541_n.jpg', 'score': 6.360086263157427e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9093_n.jpg', 'score': 6.449618376791477e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9692_n.jpg', 'score': 6.479980947915465e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10562_n.jpg', 'score': 6.485050107585266e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_768_n.jpg', 'score': 6.593175930902362e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8016_n.jpg', 'score': 6.596426828764379e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1606_n.jpg', 'score': 6.602657958865166e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4508_n.jpg', 'score': 6.604666850762442e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7296_n.jpg', 'score': 6.623386434512213e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3889_n.jpg', 'score': 6.681399827357382e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_16_n.jpg', 'score': 6.697091885143891e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4702_n.jpg', 'score': 6.727252912241966e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8684_n.jpg', 'score': 6.731115718139336e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11834_n.jpg', 'score': 6.772276537958533e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6491_n.jpg', 'score': 6.794652290409431e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_437_n.jpg', 'score': 6.798417598474771e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6953_n.jpg', 'score': 6.802730786148459e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4694_n.jpg', 'score': 6.812896754126996e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6755_n.jpg', 'score': 6.824458250775933e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11256_n.jpg', 'score': 6.855257379356772e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6300_n.jpg', 'score': 6.883097375975922e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12275_n.jpg', 'score': 6.98789008310996e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3037_n.jpg', 'score': 7.02147590345703e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7025_n.jpg', 'score': 7.026740786386654e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1718_n.jpg', 'score': 7.115145854186267e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_197_n.jpg', 'score': 7.203227141872048e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9717_n.jpg', 'score': 7.207163434941322e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3399_n.jpg', 'score': 7.228679896797985e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3203_n.jpg', 'score': 7.237136014737189e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9454_n.jpg', 'score': 7.277171243913472e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10574_n.jpg', 'score': 7.288282358786091e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4019_n.jpg', 'score': 7.339552394114435e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6140_n.jpg', 'score': 7.366717181866989e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10804_n.jpg', 'score': 7.430028927046806e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7575_n.jpg', 'score': 7.485992682632059e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5603_n.jpg', 'score': 7.553319301223382e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10809_n.jpg', 'score': 7.591785833938047e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9655_n.jpg', 'score': 7.620938413310796e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4359_n.jpg', 'score': 7.640417607035488e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11629_n.jpg', 'score': 7.674701191717759e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1474_n.jpg', 'score': 7.683745207032189e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12184_n.jpg', 'score': 7.704340532654896e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11689_n.jpg', 'score': 7.748776988591999e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3368_n.jpg', 'score': 7.81102862674743e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_1961_n.jpg', 'score': 7.811967225279659e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4340_n.jpg', 'score': 7.826917862985283e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_566_n.jpg', 'score': 7.844731590012088e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10858_n.jpg', 'score': 7.864490180509165e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10837_n.jpg', 'score': 7.899553747847676e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8640_n.jpg', 'score': 7.93733197497204e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7497_n.jpg', 'score': 7.944313256302848e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8402_n.jpg', 'score': 7.947866834001616e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4481_n.jpg', 'score': 8.027272997424006e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11618_n.jpg', 'score': 8.040427201194689e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9283_n.jpg', 'score': 8.065753354458138e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4143_n.jpg', 'score': 8.08796685305424e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_627_n.jpg', 'score': 8.126590546453372e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5373_n.jpg', 'score': 8.239370072260499e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7579_n.jpg', 'score': 8.254208660218865e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10686_n.jpg', 'score': 8.265338692581281e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8913_n.jpg', 'score': 8.27555195428431e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10283_n.jpg', 'score': 8.284371142508462e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3164_n.jpg', 'score': 8.3295235526748e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7117_n.jpg', 'score': 8.35700411698781e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6729_n.jpg', 'score': 8.377989433938637e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9871_n.jpg', 'score': 8.431350579485297e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2498_n.jpg', 'score': 8.432845788775012e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12124_n.jpg', 'score': 8.485974831273779e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8927_n.jpg', 'score': 8.490223262924701e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_598_n.jpg', 'score': 8.623535541119054e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11266_n.jpg', 'score': 8.638004510430619e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3942_n.jpg', 'score': 8.670626993989572e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6862_n.jpg', 'score': 8.709749090485275e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11290_n.jpg', 'score': 8.730263652978465e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3426_n.jpg', 'score': 8.742309728404507e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4513_n.jpg', 'score': 8.810156577965245e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11637_n.jpg', 'score': 8.854610496200621e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_3982_n.jpg', 'score': 8.879680535756052e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2834_n.jpg', 'score': 8.902249828679487e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10161_n.jpg', 'score': 8.968130714492872e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8180_n.jpg', 'score': 8.988335321191698e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10768_n.jpg', 'score': 9.046989725902677e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10388_n.jpg', 'score': 9.052927634911612e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8859_n.jpg', 'score': 9.102519834414124e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7804_n.jpg', 'score': 9.120340837398544e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_6430_n.jpg', 'score': 9.138492168858647e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_8819_n.jpg', 'score': 9.152899292530492e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4873_n.jpg', 'score': 9.154024883173406e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9791_n.jpg', 'score': 9.187562682200223e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12000_n.jpg', 'score': 9.20513630262576e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5040_n.jpg', 'score': 9.219517232850194e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_5483_n.jpg', 'score': 9.2551221314352e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_2019_n.jpg', 'score': 9.308634616900235e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_12448_n.jpg', 'score': 9.315907664131373e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_7274_n.jpg', 'score': 9.327747829956934e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_4094_n.jpg', 'score': 9.372962813358754e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_9415_n.jpg', 'score': 9.410658094566315e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_10599_n.jpg', 'score': 9.420517017133534e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11107_n.jpg', 'score': 9.444649185752496e-05}, {'file': 'data/sd_2_1_dogs_with_modifiers_small\\\\image_11434_n.jpg', 'score': 9.62374106165953e-05}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_3 = CIFAKEDataset(\"data/sd_2_1_dogs_with_modifiers_small\", num_processes=4, use_cifake_fold_structure=False)\n",
    "\n",
    "results = []\n",
    "for x, y in tqdm(dataset_3, desc=\"Evaluating images\"):\n",
    "    x = x.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model_2(x).squeeze().item()\n",
    "        file = dataset_3.latest_file_path\n",
    "        results.append({\"file\": file, \"score\": output})\n",
    "\n",
    "# Sort the list of dictionaries by the \"score\" key\n",
    "sorted_results = sorted(results, key=lambda x: x['score'])\n",
    "most_fakes = sorted_results[:200]\n",
    "print(most_fakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47ae33",
   "metadata": {},
   "source": [
    "### Lora Training Command\n",
    "`accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" --dataset_name=\"\\workspaces\\cifake_classification\\data\\most_fake_dogs\" --resolution=768 --center_crop --random_flip --train_batch_size=1 --gradient_accumulation_steps=4 --max_train_steps=15000 --learning_rate=1e-04 --max_grad_norm=1 --lr_scheduler=\"cosine\" --lr_warmup_steps=0 --output_dir=\"\\workspaces\\cifake_classification\\data\\lora_result\" --checkpointing_steps=500  --validation_prompt=\"a photograph of a dog, f4ke\" --seed=1337`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e64706d-3e47-4588-bdd9-a6c6aa543360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 501/501 [00:02<00:00, 190.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_lora_dogs_with_modifiers\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_lora_dogs_with_modifiers\\\\test.py'\n",
      "Test the model with lora images with negative prompt 95.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 273.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_lora_dogs_with_modifiers_no_trigger\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_lora_dogs_with_modifiers_no_trigger\\\\test.py'\n",
      " Test the model with lora images without negative prompt 97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_lora_dogs_with_modifiers\")\n",
    "print(f\"Test the model with lora images with negative prompt {s}\")\n",
    "# Step 2: Test the model with original real images\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_lora_dogs_with_modifiers_no_trigger\")\n",
    "print(f\" Test the model with lora images without negative prompt {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc3f52c-e989-40c1-b235-551a685a4a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 269.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_lora_dogs_with_modifiers_no_trigger_negative\\\\test.py'\n",
      " Test the model with lora images without negative prompt but with negative setting 88.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 268.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_2\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_2\\\\test.py'\n",
      " Test the model with lora images without negative prompt but with negative setting v2 81.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 501/501 [00:01<00:00, 301.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to load one image data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_3\\test.py, Error: cannot identify image file 'D:\\\\workspaces\\\\cifake_classification\\\\data\\\\sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_3\\\\test.py'\n",
      " Test the model with lora images without negative prompt but with negative setting v3 69.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative\")\n",
    "print(f\" Test the model with lora images without negative prompt but with negative setting {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_2\")\n",
    "print(f\" Test the model with lora images without negative prompt but with negative setting v2 {s}\")\n",
    "_, _, s,_ = evaluate_folder(model_2, \"data/sd_2_1_lora_dogs_with_modifiers_no_trigger_negative_3\")\n",
    "print(f\" Test the model with lora images without negative prompt but with negative setting v3 {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69592790-d29f-4f5d-9eb9-924a81ffd973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
